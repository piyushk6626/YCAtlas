{
  "links": "https://www.ycombinator.com/companies/ncompass-technologies",
  "name": "nCompass Technologies",
  "headline": "Deploy hardware accelerated AI models with only one line of code",
  "batch": "W24",
  "description": "nCompass is a platform for acceleration and hosting of open-source and custom AI models. We provide low-latency AI deployment without rate-limiting you. All with just one line of code.",
  "activity_status": "Active",
  "website": "https://www.ncompass.tech",
  "founded_date": 2023.0,
  "team_size": 2.0,
  "location": null,
  "group_partner": "Dalton Caldwell",
  "group_partner_yc": "https://www.ycombinator.com/people/dalton-caldwell",
  "company_linkedin": null,
  "company_twitter": null,
  "tags": "industry:hardware; industry:open-source; industry:api; industry:cloud-computing; industry:ai",
  "founders": [
    {
      "name": "Aditya Rajagopal, Founder",
      "description": "I am a recent PhD graduate from Imperial College London with experience in machine learning algorithms, compilers and hardware architectures. I've worked in compiler teams at Qualcomm and Huawei as well as served as a reviewer for ICML. \n\nMy co-founder and I are building nCompass which is a platform for accelerating and hosting both open-source and custom large AI models. Our focus is on providing rate unlimited and low latency large AI inference with only one line of code.",
      "linkedin": "https://linkedin.com/in/adityarajagopal"
    },
    {
      "name": "Diederik Vink, Founder",
      "description": "I'm a recent Imperial College London PhD Graduate where I specialized in reconfigurable hardware architectures for accelerated machine learning and reduced precision training algorithms. I have worked as an AI feasibility consultant prototyping and evaluating AI spin-outs.\nWe are building nCompass, a platform for accelerating and hosting both open-source and custom large AI models. Our focus is on providing rate-unlimited and low latency large AI inference with only one line of code.",
      "linkedin": "https://linkedin.com/in/diederik-vink"
    }
  ],
  "status": true,
  "markdown": "raw_markdown='[![nCompass Technologies](https://www.ncompass.tech/lovable-uploads/7c815a91-8abf-41e3-8819-8f20d3ee7b9e.png)](https://www.ncompass.tech/</>)\\n[Performance](https://www.ncompass.tech/</#benchmarks>)[Products](https://www.ncompass.tech/</#products-section>)[Blog](https://www.ncompass.tech/</blog>)[About Us](https://www.ncompass.tech/</about>)Book a Demo\\nOpen main menu\\n[Performance](https://www.ncompass.tech/</#benchmarks>)[Products](https://www.ncompass.tech/</#products>)[Blog](https://www.ncompass.tech/</blog>)[About Us](https://www.ncompass.tech/</about>)Book a Demo\\nOptimize your A\\nncompass.sglang.llama.attn()\\n### Upto 3.5x Higher Throughput\\nDrop-in replacement for torch.nn with model-specific optimizations\\n### Zero Migration\\nIntegrates with your existing inference engine stack\\nExplore Products\\nBook a Demo\\n## Change one line of code in your existing framework and watch your code speedup\\nvLLMLlamamlpSee Diff\\nvllm/model_executor/models/llama.py\\n@@ -254,1 +61,2 @@\\n...\\n-254self.mlp = LlamaMLP(\\n+61import ncompass\\n+254self.mlp = ncompass.vllm.llama.MLP(\\n...\\n20%\\nThroughput Improvement\\nLoading models...\\n## Products\\n### License our optimized GPU kernels\\nTap into our library of optimized kernels to accelerate your deployment.\\n  * Works on any AI model\\n  * Bring your own custom AI model architecture\\n  * Works on any hardware backend\\n\\n\\nBook a call\\n### Deploy a dedicated instance\\nLet us manage your AI inference infrastructure\\n  * Dedicated: Deploy on our H100 GPUs\\n  * On-prem : Deploy on your infrastructure\\n  * We provide autoscaling, observability and more...\\n\\n\\nBook a call\\n### Try us out on our API\\nRun AI models without any rate limits\\n  * Limited set of optimized open source models\\n  * 40s cold starts for 8B and 70B\\n  * OpenAI compatible API interface\\n\\n\\nRun your first query\\n### Navigation\\nPerformanceProductsBlogAbout Us\\n### Connect With Us\\n[LinkedIn](https://www.ncompass.tech/<https:/www.linkedin.com/company/ncompass-technologies/>)[X (Twitter)](https://www.ncompass.tech/<https:/x.com/nCompass_tech>)[Substack](https://www.ncompass.tech/<https:/ncompasstech.substack.com/>)Email\\n### nCompass Technologies\\n© 2025 nCompass Technologies.All rights reserved.\\n' markdown_with_citations='![nCompass Technologies⟨1⟩](https://www.ncompass.tech/</>)\\nPerformance⟨2⟩Products⟨3⟩Blog⟨4⟩About Us⟨5⟩Book a Demo\\nOpen main menu\\nPerformance⟨2⟩Products⟨6⟩Blog⟨4⟩About Us⟨5⟩Book a Demo\\nOptimize your A\\nncompass.sglang.llama.attn()\\n### Upto 3.5x Higher Throughput\\nDrop-in replacement for torch.nn with model-specific optimizations\\n### Zero Migration\\nIntegrates with your existing inference engine stack\\nExplore Products\\nBook a Demo\\n## Change one line of code in your existing framework and watch your code speedup\\nvLLMLlamamlpSee Diff\\nvllm/model_executor/models/llama.py\\n@@ -254,1 +61,2 @@\\n...\\n-254self.mlp = LlamaMLP(\\n+61import ncompass\\n+254self.mlp = ncompass.vllm.llama.MLP(\\n...\\n20%\\nThroughput Improvement\\nLoading models...\\n## Products\\n### License our optimized GPU kernels\\nTap into our library of optimized kernels to accelerate your deployment.\\n  * Works on any AI model\\n  * Bring your own custom AI model architecture\\n  * Works on any hardware backend\\n\\n\\nBook a call\\n### Deploy a dedicated instance\\nLet us manage your AI inference infrastructure\\n  * Dedicated: Deploy on our H100 GPUs\\n  * On-prem : Deploy on your infrastructure\\n  * We provide autoscaling, observability and more...\\n\\n\\nBook a call\\n### Try us out on our API\\nRun AI models without any rate limits\\n  * Limited set of optimized open source models\\n  * 40s cold starts for 8B and 70B\\n  * OpenAI compatible API interface\\n\\n\\nRun your first query\\n### Navigation\\nPerformanceProductsBlogAbout Us\\n### Connect With Us\\nLinkedIn⟨7⟩X (Twitter)⟨8⟩Substack⟨9⟩Email\\n### nCompass Technologies\\n© 2025 nCompass Technologies.All rights reserved.\\n' references_markdown='\\n\\n## References\\n\\n⟨1⟩ https://www.ncompass.tech/lovable-uploads/7c815a91-8abf-41e3-8819-8f20d3ee7b9e.png: ![nCompass Technologies\\n⟨2⟩ https://www.ncompass.tech/</#benchmarks>: Performance\\n⟨3⟩ https://www.ncompass.tech/</#products-section>: Products\\n⟨4⟩ https://www.ncompass.tech/</blog>: Blog\\n⟨5⟩ https://www.ncompass.tech/</about>: About Us\\n⟨6⟩ https://www.ncompass.tech/</#products>: Products\\n⟨7⟩ https://www.ncompass.tech/<https:/www.linkedin.com/company/ncompass-technologies/>: LinkedIn\\n⟨8⟩ https://www.ncompass.tech/<https:/x.com/nCompass_tech>: X (Twitter)\\n⟨9⟩ https://www.ncompass.tech/<https:/ncompasstech.substack.com/>: Substack\\n' fit_markdown='' fit_html=''",
  "generated_description": "**nCompass Technologies: Simplifying AI Deployment with Just One Line of Code**\n\nFounded in 2023 and now part of Y Combinator's Batch W24, nCompass Technologies is revolutionizing the way we deploy AI models. The company provides a platform that accelerates and hosts both open-source and custom AI models, aiming for low-latency deployment without restrictions.\n\n### Mission\nThe core mission of nCompass is straightforward yet ambitious: to enable users to deploy hardware-accelerated AI models using only one line of code. This simplicity is a game-changer for developers who need speed and efficiency in their AI applications.\n\n### What They Do\nnCompass offers several key services:\n- **Optimized GPU Kernels**: Their library of optimized kernels works with any AI model, allowing users to bring their own custom architecture and deploy across various hardware backends.\n- **Dedicated Instances**: Users can choose to run their AI inference infrastructure either on nCompass's high-performance H100 GPUs or on their own infrastructure, complete with autoscaling and observability.\n- **API Access**: The nCompass API allows for unrestricted access to a selection of optimized open-source models, all compliant with OpenAI's API interface.\n\n### Performance Highlights\nnCompass boasts significant performance improvements with claims of up to 3.5 times higher throughput. Their technology seamlessly integrates with existing frameworks, ensuring that users can optimize their models without hassle.\n\n### Team and Vision\nCurrently a small but potent team of just two, nCompass is supported by Dalton Caldwell as their Group Partner. The small size allows for agile decision-making and a focused approach to problem-solving in the AI deployment landscape.\n\n### Keywords\nThe key themes and technologies guiding nCompass include: \n- Hardware Acceleration\n- Open-Source Solutions\n- API Development\n- Cloud Computing\n- Artificial Intelligence\n\n### Conclusion\nFor developers and companies looking to streamline their AI deployment processes, nCompass Technologies presents a compelling solution. With their innovative platform, users can transform their existing frameworks and optimize their AI capabilities simply and effectively.\n\nFor more information, visit their [website](https://www.ncompass.tech)."
}