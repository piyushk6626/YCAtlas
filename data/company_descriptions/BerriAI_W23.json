{
  "links": "https://www.ycombinator.com/companies/berriai",
  "name": "BerriAI",
  "headline": "Call every LLM API like it's OpenAI [100+ LLMs]",
  "batch": "W23",
  "description": "An open-source library to simplify LLM completion + embedding calls: https://github.com/BerriAI/litellm",
  "activity_status": "Active",
  "website": "https://github.com/BerriAI/litellm",
  "founded_date": 2023.0,
  "team_size": 2.0,
  "location": null,
  "group_partner": "Aaron Epstein",
  "group_partner_yc": "https://www.ycombinator.com/people/aaron-epstein",
  "company_linkedin": null,
  "company_twitter": null,
  "tags": null,
  "founders": [],
  "status": true,
  "markdown": "raw_markdown='[Skip to content](https://github.com/BerriAI/<#start-of-content>)\\n## Navigation Menu\\nToggle navigation\\n[ ](https://github.com/BerriAI/</>)\\n[ Sign in ](https://github.com/BerriAI/</login?return_to=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm>)\\n  * Product \\n    * [ GitHub Copilot Write better code with AI  ](https://github.com/BerriAI/<https:/github.com/features/copilot>)\\n    * [ Security Find and fix vulnerabilities  ](https://github.com/BerriAI/<https:/github.com/features/security>)\\n    * [ Actions Automate any workflow  ](https://github.com/BerriAI/<https:/github.com/features/actions>)\\n    * [ Codespaces Instant dev environments  ](https://github.com/BerriAI/<https:/github.com/features/codespaces>)\\n    * [ Issues Plan and track work  ](https://github.com/BerriAI/<https:/github.com/features/issues>)\\n    * [ Code Review Manage code changes  ](https://github.com/BerriAI/<https:/github.com/features/code-review>)\\n    * [ Discussions Collaborate outside of code  ](https://github.com/BerriAI/<https:/github.com/features/discussions>)\\n    * [ Code Search Find more, search less  ](https://github.com/BerriAI/<https:/github.com/features/code-search>)\\nExplore\\n    * [ All features ](https://github.com/BerriAI/<https:/github.com/features>)\\n    * [ Documentation ](https://github.com/BerriAI/<https:/docs.github.com>)\\n    * [ GitHub Skills ](https://github.com/BerriAI/<https:/skills.github.com>)\\n    * [ Blog ](https://github.com/BerriAI/<https:/github.blog>)\\n  * Solutions \\nBy company size\\n    * [ Enterprises ](https://github.com/BerriAI/<https:/github.com/enterprise>)\\n    * [ Small and medium teams ](https://github.com/BerriAI/<https:/github.com/team>)\\n    * [ Startups ](https://github.com/BerriAI/<https:/github.com/enterprise/startups>)\\n    * [ Nonprofits ](https://github.com/BerriAI/</solutions/industry/nonprofits>)\\nBy use case\\n    * [ DevSecOps ](https://github.com/BerriAI/</solutions/use-case/devsecops>)\\n    * [ DevOps ](https://github.com/BerriAI/</solutions/use-case/devops>)\\n    * [ CI/CD ](https://github.com/BerriAI/</solutions/use-case/ci-cd>)\\n    * [ View all use cases ](https://github.com/BerriAI/</solutions/use-case>)\\nBy industry\\n    * [ Healthcare ](https://github.com/BerriAI/</solutions/industry/healthcare>)\\n    * [ Financial services ](https://github.com/BerriAI/</solutions/industry/financial-services>)\\n    * [ Manufacturing ](https://github.com/BerriAI/</solutions/industry/manufacturing>)\\n    * [ Government ](https://github.com/BerriAI/</solutions/industry/government>)\\n    * [ View all industries ](https://github.com/BerriAI/</solutions/industry>)\\n[ View all solutions ](https://github.com/BerriAI/</solutions>)\\n  * Resources \\nTopics\\n    * [ AI ](https://github.com/BerriAI/</resources/articles/ai>)\\n    * [ DevOps ](https://github.com/BerriAI/</resources/articles/devops>)\\n    * [ Security ](https://github.com/BerriAI/</resources/articles/security>)\\n    * [ Software Development ](https://github.com/BerriAI/</resources/articles/software-development>)\\n    * [ View all ](https://github.com/BerriAI/</resources/articles>)\\nExplore\\n    * [ Learning Pathways ](https://github.com/BerriAI/<https:/resources.github.com/learn/pathways>)\\n    * [ White papers, Ebooks, Webinars ](https://github.com/BerriAI/<https:/resources.github.com>)\\n    * [ Customer Stories ](https://github.com/BerriAI/<https:/github.com/customer-stories>)\\n    * [ Partners ](https://github.com/BerriAI/<https:/partner.github.com>)\\n    * [ Executive Insights ](https://github.com/BerriAI/<https:/github.com/solutions/executive-insights>)\\n  * Open Source \\n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/BerriAI/</sponsors>)\\n    * [ The ReadME Project GitHub community articles  ](https://github.com/BerriAI/<https:/github.com/readme>)\\nRepositories\\n    * [ Topics ](https://github.com/BerriAI/<https:/github.com/topics>)\\n    * [ Trending ](https://github.com/BerriAI/<https:/github.com/trending>)\\n    * [ Collections ](https://github.com/BerriAI/<https:/github.com/collections>)\\n  * Enterprise \\n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/BerriAI/</enterprise>)\\nAvailable add-ons\\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/BerriAI/<https:/github.com/enterprise/advanced-security>)\\n    * [ GitHub Copilot Enterprise-grade AI features  ](https://github.com/BerriAI/</features/copilot#enterprise>)\\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/BerriAI/</premium-support>)\\n  * [Pricing](https://github.com/BerriAI/<https:/github.com/pricing>)\\n\\n\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\nSearch \\nClear\\n[Search syntax tips](https://github.com/BerriAI/<https:/docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax>)\\n#  Provide feedback \\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancel  Submit feedback \\n#  Saved searches \\n## Use saved searches to filter your results more quickly\\nName\\nQuery\\nTo see all available qualifiers, see our [documentation](https://github.com/BerriAI/<https:/docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax>). \\nCancel  Create saved search \\n[ Sign in ](https://github.com/BerriAI/</login?return_to=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm>)\\n[ Sign up ](https://github.com/BerriAI/</signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=BerriAI%2Flitellm>) Reseting focus\\nYou signed in with another tab or window. [Reload](https://github.com/BerriAI/<>) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/BerriAI/<>) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/BerriAI/<>) to refresh your session. Dismiss alert\\n{{ message }}\\n[ BerriAI ](https://github.com/BerriAI/</BerriAI>) / **[litellm](https://github.com/BerriAI/</BerriAI/litellm>) ** Public\\n  * [ Notifications ](https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>) You must be signed in to change notification settings\\n  * [ Fork 2.2k ](https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>)\\n  * [ Star  17.7k ](https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>)\\n\\n\\nPython SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq] \\n[docs.litellm.ai/docs/](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/> \"https://docs.litellm.ai/docs/\")\\n### License\\n[ View license ](https://github.com/BerriAI/</BerriAI/litellm/blob/main/LICENSE>)\\n[ 17.7k stars ](https://github.com/BerriAI/</BerriAI/litellm/stargazers>) [ 2.2k forks ](https://github.com/BerriAI/</BerriAI/litellm/forks>) [ Branches ](https://github.com/BerriAI/</BerriAI/litellm/branches>) [ Tags ](https://github.com/BerriAI/</BerriAI/litellm/tags>) [ Activity ](https://github.com/BerriAI/</BerriAI/litellm/activity>)\\n[ Star  ](https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>)\\n[ Notifications ](https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>) You must be signed in to change notification settings\\n  * [ Code ](https://github.com/BerriAI/</BerriAI/litellm>)\\n  * [ Issues 948 ](https://github.com/BerriAI/</BerriAI/litellm/issues>)\\n  * [ Pull requests 300 ](https://github.com/BerriAI/</BerriAI/litellm/pulls>)\\n  * [ Discussions ](https://github.com/BerriAI/</BerriAI/litellm/discussions>)\\n  * [ Actions ](https://github.com/BerriAI/</BerriAI/litellm/actions>)\\n  * [ Projects 0 ](https://github.com/BerriAI/</BerriAI/litellm/projects>)\\n  * [ Security ](https://github.com/BerriAI/</BerriAI/litellm/security>)\\n  * [ Insights ](https://github.com/BerriAI/</BerriAI/litellm/pulse>)\\n\\n\\nAdditional navigation options\\n  * [ Code  ](https://github.com/BerriAI/</BerriAI/litellm>)\\n  * [ Issues  ](https://github.com/BerriAI/</BerriAI/litellm/issues>)\\n  * [ Pull requests  ](https://github.com/BerriAI/</BerriAI/litellm/pulls>)\\n  * [ Discussions  ](https://github.com/BerriAI/</BerriAI/litellm/discussions>)\\n  * [ Actions  ](https://github.com/BerriAI/</BerriAI/litellm/actions>)\\n  * [ Projects  ](https://github.com/BerriAI/</BerriAI/litellm/projects>)\\n  * [ Security  ](https://github.com/BerriAI/</BerriAI/litellm/security>)\\n  * [ Insights  ](https://github.com/BerriAI/</BerriAI/litellm/pulse>)\\n\\n\\n# BerriAI/litellm\\nmain\\n[Branches](https://github.com/BerriAI/</BerriAI/litellm/branches>)[Tags](https://github.com/BerriAI/</BerriAI/litellm/tags>)\\n[](https://github.com/BerriAI/</BerriAI/litellm/branches>)[](https://github.com/BerriAI/</BerriAI/litellm/tags>)\\nGo to file\\nCode\\n## Folders and files\\nName| Name| Last commit message| Last commit date  \\n---|---|---|---  \\n## Latest commit\\n## History\\n[19,542 Commits](https://github.com/BerriAI/</BerriAI/litellm/commits/main/>)[](https://github.com/BerriAI/</BerriAI/litellm/commits/main/>)  \\n[.circleci](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.circleci> \".circleci\")| [.circleci](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.circleci> \".circleci\")  \\n[.devcontainer](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.devcontainer> \".devcontainer\")| [.devcontainer](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.devcontainer> \".devcontainer\")  \\n[.github](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.github> \".github\")| [.github](https://github.com/BerriAI/</BerriAI/litellm/tree/main/.github> \".github\")  \\n[ci_cd](https://github.com/BerriAI/</BerriAI/litellm/tree/main/ci_cd> \"ci_cd\")| [ci_cd](https://github.com/BerriAI/</BerriAI/litellm/tree/main/ci_cd> \"ci_cd\")  \\n[cookbook](https://github.com/BerriAI/</BerriAI/litellm/tree/main/cookbook> \"cookbook\")| [cookbook](https://github.com/BerriAI/</BerriAI/litellm/tree/main/cookbook> \"cookbook\")  \\n[db_scripts](https://github.com/BerriAI/</BerriAI/litellm/tree/main/db_scripts> \"db_scripts\")| [db_scripts](https://github.com/BerriAI/</BerriAI/litellm/tree/main/db_scripts> \"db_scripts\")  \\n[deploy](https://github.com/BerriAI/</BerriAI/litellm/tree/main/deploy> \"deploy\")| [deploy](https://github.com/BerriAI/</BerriAI/litellm/tree/main/deploy> \"deploy\")  \\n[dist](https://github.com/BerriAI/</BerriAI/litellm/tree/main/dist> \"dist\")| [dist](https://github.com/BerriAI/</BerriAI/litellm/tree/main/dist> \"dist\")  \\n[docker](https://github.com/BerriAI/</BerriAI/litellm/tree/main/docker> \"docker\")| [docker](https://github.com/BerriAI/</BerriAI/litellm/tree/main/docker> \"docker\")  \\n[docs/my-website](https://github.com/BerriAI/</BerriAI/litellm/tree/main/docs/my-website> \"This path skips through empty directories\")| [docs/my-website](https://github.com/BerriAI/</BerriAI/litellm/tree/main/docs/my-website> \"This path skips through empty directories\")  \\n[enterprise](https://github.com/BerriAI/</BerriAI/litellm/tree/main/enterprise> \"enterprise\")| [enterprise](https://github.com/BerriAI/</BerriAI/litellm/tree/main/enterprise> \"enterprise\")  \\n[litellm-js](https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm-js> \"litellm-js\")| [litellm-js](https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm-js> \"litellm-js\")  \\n[litellm](https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm> \"litellm\")| [litellm](https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm> \"litellm\")  \\n[tests](https://github.com/BerriAI/</BerriAI/litellm/tree/main/tests> \"tests\")| [tests](https://github.com/BerriAI/</BerriAI/litellm/tree/main/tests> \"tests\")  \\n[ui](https://github.com/BerriAI/</BerriAI/litellm/tree/main/ui> \"ui\")| [ui](https://github.com/BerriAI/</BerriAI/litellm/tree/main/ui> \"ui\")  \\n[.dockerignore](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.dockerignore> \".dockerignore\")| [.dockerignore](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.dockerignore> \".dockerignore\")  \\n[.env.example](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.env.example> \".env.example\")| [.env.example](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.env.example> \".env.example\")  \\n[.flake8](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.flake8> \".flake8\")| [.flake8](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.flake8> \".flake8\")  \\n[.git-blame-ignore-revs](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.git-blame-ignore-revs> \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.git-blame-ignore-revs> \".git-blame-ignore-revs\")  \\n[.gitattributes](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitattributes> \".gitattributes\")| [.gitattributes](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitattributes> \".gitattributes\")  \\n[.gitignore](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitignore> \".gitignore\")| [.gitignore](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitignore> \".gitignore\")  \\n[.pre-commit-config.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.pre-commit-config.yaml> \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/.pre-commit-config.yaml> \".pre-commit-config.yaml\")  \\n[Dockerfile](https://github.com/BerriAI/</BerriAI/litellm/blob/main/Dockerfile> \"Dockerfile\")| [Dockerfile](https://github.com/BerriAI/</BerriAI/litellm/blob/main/Dockerfile> \"Dockerfile\")  \\n[LICENSE](https://github.com/BerriAI/</BerriAI/litellm/blob/main/LICENSE> \"LICENSE\")| [LICENSE](https://github.com/BerriAI/</BerriAI/litellm/blob/main/LICENSE> \"LICENSE\")  \\n[README.md](https://github.com/BerriAI/</BerriAI/litellm/blob/main/README.md> \"README.md\")| [README.md](https://github.com/BerriAI/</BerriAI/litellm/blob/main/README.md> \"README.md\")  \\n[codecov.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/codecov.yaml> \"codecov.yaml\")| [codecov.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/codecov.yaml> \"codecov.yaml\")  \\n[docker-compose.yml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/docker-compose.yml> \"docker-compose.yml\")| [docker-compose.yml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/docker-compose.yml> \"docker-compose.yml\")  \\n[index.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/index.yaml> \"index.yaml\")| [index.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/index.yaml> \"index.yaml\")  \\n[model_prices_and_context_window.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/model_prices_and_context_window.json> \"model_prices_and_context_window.json\")| [model_prices_and_context_window.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/model_prices_and_context_window.json> \"model_prices_and_context_window.json\")  \\n[mypy.ini](https://github.com/BerriAI/</BerriAI/litellm/blob/main/mypy.ini> \"mypy.ini\")| [mypy.ini](https://github.com/BerriAI/</BerriAI/litellm/blob/main/mypy.ini> \"mypy.ini\")  \\n[package-lock.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/package-lock.json> \"package-lock.json\")| [package-lock.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/package-lock.json> \"package-lock.json\")  \\n[package.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/package.json> \"package.json\")| [package.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/package.json> \"package.json\")  \\n[poetry.lock](https://github.com/BerriAI/</BerriAI/litellm/blob/main/poetry.lock> \"poetry.lock\")| [poetry.lock](https://github.com/BerriAI/</BerriAI/litellm/blob/main/poetry.lock> \"poetry.lock\")  \\n[prometheus.yml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/prometheus.yml> \"prometheus.yml\")| [prometheus.yml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/prometheus.yml> \"prometheus.yml\")  \\n[proxy_server_config.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/proxy_server_config.yaml> \"proxy_server_config.yaml\")| [proxy_server_config.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/proxy_server_config.yaml> \"proxy_server_config.yaml\")  \\n[pyproject.toml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyproject.toml> \"pyproject.toml\")| [pyproject.toml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyproject.toml> \"pyproject.toml\")  \\n[pyrightconfig.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyrightconfig.json> \"pyrightconfig.json\")| [pyrightconfig.json](https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyrightconfig.json> \"pyrightconfig.json\")  \\n[render.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/render.yaml> \"render.yaml\")| [render.yaml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/render.yaml> \"render.yaml\")  \\n[requirements.txt](https://github.com/BerriAI/</BerriAI/litellm/blob/main/requirements.txt> \"requirements.txt\")| [requirements.txt](https://github.com/BerriAI/</BerriAI/litellm/blob/main/requirements.txt> \"requirements.txt\")  \\n[ruff.toml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/ruff.toml> \"ruff.toml\")| [ruff.toml](https://github.com/BerriAI/</BerriAI/litellm/blob/main/ruff.toml> \"ruff.toml\")  \\n[schema.prisma](https://github.com/BerriAI/</BerriAI/litellm/blob/main/schema.prisma> \"schema.prisma\")| [schema.prisma](https://github.com/BerriAI/</BerriAI/litellm/blob/main/schema.prisma> \"schema.prisma\")  \\n[security.md](https://github.com/BerriAI/</BerriAI/litellm/blob/main/security.md> \"security.md\")| [security.md](https://github.com/BerriAI/</BerriAI/litellm/blob/main/security.md> \"security.md\")  \\nView all files  \\n## Repository files navigation\\n  * [README](https://github.com/BerriAI/<#>)\\n  * [License](https://github.com/BerriAI/<#>)\\n  * [Security](https://github.com/BerriAI/<#>)\\n\\n\\n#  🚅 LiteLLM \\n[](https://github.com/BerriAI/<#---------litellm---->)\\n[![Deploy to Render](https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667)](https://github.com/BerriAI/<https:/render.com/deploy?repo=https://github.com/BerriAI/litellm>) [ ![Deploy on Railway](https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667) ](https://github.com/BerriAI/<https:/railway.app/template/HLP0Ub?referralCode=jch2ME>)\\nCall all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] \\n#### [LiteLLM Proxy Server (LLM Gateway)](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/simple_proxy>) | [ Hosted Proxy (Preview)](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/hosted>) | [Enterprise Tier](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/enterprise>)\\n[](https://github.com/BerriAI/<#litellm-proxy-server-llm-gateway---hosted-proxy-preview--enterprise-tier>)\\n####  [ ![PyPI Version](https://camo.githubusercontent.com/de190803172c4d35f85e73a0f4eec265b5029bb0ad250f402aac9ca1bd73bd79/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6974656c6c6d2e737667) ](https://github.com/BerriAI/<https:/pypi.org/project/litellm/>) [ ![CircleCI](https://camo.githubusercontent.com/ef06f9362d95c52b7b1dc33f5ff1817c1575fa6e9881847927bccb92c6e063e8/68747470733a2f2f646c2e636972636c6563692e636f6d2f7374617475732d62616467652f696d672f67682f426572726941492f6c6974656c6c6d2f747265652f6d61696e2e7376673f7374796c653d737667) ](https://github.com/BerriAI/<https:/dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main>) [ ![Y Combinator W23](https://camo.githubusercontent.com/e1e0029e353d103690da84a20e88b7051eebbcdede2a1b35d9d1b78b0b0295cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732332d6f72616e67653f7374796c653d666c61742d737175617265) ](https://github.com/BerriAI/<https:/www.ycombinator.com/companies/berriai>) [ ![Whatsapp](https://camo.githubusercontent.com/78382e0d13839fedd81996b3e7cbecea33222e5ea36d54d07455a93dfd68e5d7/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d576861747341707026636f6c6f723d73756363657373266c6f676f3d5768617473417070267374796c653d666c61742d737175617265) ](https://github.com/BerriAI/<https:/wa.link/huol9n>) [ ![Discord](https://camo.githubusercontent.com/bcba2d72b7345e8de3adc1f330b340b72f37842dd275a91c4f31154e23cc8cd0/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d446973636f726426636f6c6f723d626c7565266c6f676f3d446973636f7264267374796c653d666c61742d737175617265) ](https://github.com/BerriAI/<https:/discord.gg/wuPM9dRgDw>)\\n[](https://github.com/BerriAI/<#-------------------------------------------------------------------------------->)\\nLiteLLM manages:\\n  * Translate inputs to provider\\'s `completion`, `embedding`, and `image_generation` endpoints\\n  * [Consistent output](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/output>), text responses will always be available at `[\\'choices\\'][0][\\'message\\'][\\'content\\']`\\n  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/routing>)\\n  * Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/simple_proxy>)\\n\\n\\n[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs>) [**Jump to Supported LLM Providers**](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs>)\\n🚨 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published.\\nSupport for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+>).\\n# Usage ([**Docs**](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/>))\\n[](https://github.com/BerriAI/<#usage-docs>)\\nImportant\\nLiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide [here](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/migration>) LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.\\n[ ![Open In Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667) ](https://github.com/BerriAI/<https:/colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb>)\\n```\\npip install litellm\\n```\\n\\n```\\nfrom litellm import completion\\nimport os\\n## set ENV variables\\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\\nmessages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\\n# openai call\\nresponse = completion(model=\"openai/gpt-4o\", messages=messages)\\n# anthropic call\\nresponse = completion(model=\"anthropic/claude-3-sonnet-20240229\", messages=messages)\\nprint(response)\\n```\\n\\n### Response (OpenAI Format)\\n[](https://github.com/BerriAI/<#response-openai-format>)\\n```\\n{\\n  \"id\": \"chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885\",\\n  \"created\": 1734366691,\\n  \"model\": \"claude-3-sonnet-20240229\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"message\": {\\n        \"content\": \"Hello! As an AI language model, I don\\'t have feelings, but I\\'m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\",\\n        \"role\": \"assistant\",\\n        \"tool_calls\": null,\\n        \"function_call\": null\\n      }\\n    }\\n  ],\\n  \"usage\": {\\n    \"completion_tokens\": 43,\\n    \"prompt_tokens\": 13,\\n    \"total_tokens\": 56,\\n    \"completion_tokens_details\": null,\\n    \"prompt_tokens_details\": {\\n      \"audio_tokens\": null,\\n      \"cached_tokens\": 0\\n    },\\n    \"cache_creation_input_tokens\": 0,\\n    \"cache_read_input_tokens\": 0\\n  }\\n}\\n```\\n\\nCall any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers>)\\n## Async ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#async-completion>))\\n[](https://github.com/BerriAI/<#async-docs>)\\n```\\nfrom litellm import acompletion\\nimport asyncio\\nasync def test_get_response():\\n  user_message = \"Hello, how are you?\"\\n  messages = [{\"content\": user_message, \"role\": \"user\"}]\\n  response = await acompletion(model=\"openai/gpt-4o\", messages=messages)\\n  return response\\nresponse = asyncio.run(test_get_response())\\nprint(response)\\n```\\n\\n## Streaming ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream>))\\n[](https://github.com/BerriAI/<#streaming-docs>)\\nliteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response. Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)\\n```\\nfrom litellm import completion\\nresponse = completion(model=\"openai/gpt-4o\", messages=messages, stream=True)\\nfor part in response:\\n  print(part.choices[0].delta.content or \"\")\\n# claude 2\\nresponse = completion(\\'anthropic/claude-3-sonnet-20240229\\', messages, stream=True)\\nfor part in response:\\n  print(part)\\n```\\n\\n### Response chunk (OpenAI Format)\\n[](https://github.com/BerriAI/<#response-chunk-openai-format>)\\n```\\n{\\n  \"id\": \"chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697\",\\n  \"created\": 1734366925,\\n  \"model\": \"claude-3-sonnet-20240229\",\\n  \"object\": \"chat.completion.chunk\",\\n  \"system_fingerprint\": null,\\n  \"choices\": [\\n    {\\n      \"finish_reason\": null,\\n      \"index\": 0,\\n      \"delta\": {\\n        \"content\": \"Hello\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null,\\n        \"audio\": null\\n      },\\n      \"logprobs\": null\\n    }\\n  ]\\n}\\n```\\n\\n## Logging Observability ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/observability/callbacks>))\\n[](https://github.com/BerriAI/<#logging-observability-docs>)\\nLiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack\\n```\\nfrom litellm import completion\\n## set env variables for logging tools (when using MLflow, no API key set up is required)\\nos.environ[\"LUNARY_PUBLIC_KEY\"] = \"your-lunary-public-key\"\\nos.environ[\"HELICONE_API_KEY\"] = \"your-helicone-auth-key\"\\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\\nos.environ[\"ATHINA_API_KEY\"] = \"your-athina-api-key\"\\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\\n# set callbacks\\nlitellm.success_callback = [\"lunary\", \"mlflow\", \"langfuse\", \"athina\", \"helicone\"] # log input/output to lunary, langfuse, supabase, athina, helicone etc\\n#openai call\\nresponse = completion(model=\"openai/gpt-4o\", messages=[{\"role\": \"user\", \"content\": \"Hi 👋 - i\\'m openai\"}])\\n```\\n\\n# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/simple_proxy>))\\n[](https://github.com/BerriAI/<#litellm-proxy-server-llm-gateway---docs>)\\nTrack spend + Load Balance across multiple projects\\n[Hosted Proxy (Preview)](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/hosted>)\\nThe proxy provides:\\n  1. [Hooks for auth](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys#custom-auth>)\\n  2. [Hooks for logging](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class>)\\n  3. [Cost tracking](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend>)\\n  4. [Rate Limiting](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/users#set-rate-limits>)\\n\\n\\n## 📖 Proxy Endpoints - [Swagger Docs](https://github.com/BerriAI/<https:/litellm-api.up.railway.app/>)\\n[](https://github.com/BerriAI/<#-proxy-endpoints---swagger-docs>)\\n## Quick Start Proxy - CLI\\n[](https://github.com/BerriAI/<#quick-start-proxy---cli>)\\n```\\npip install \\'litellm[proxy]\\'\\n```\\n\\n### Step 1: Start litellm proxy\\n[](https://github.com/BerriAI/<#step-1-start-litellm-proxy>)\\n```\\n$ litellm --model huggingface/bigcode/starcoder\\n#INFO: Proxy running on http://0.0.0.0:4000\\n```\\n\\n### Step 2: Make ChatCompletions Request to Proxy\\n[](https://github.com/BerriAI/<#step-2-make-chatcompletions-request-to-proxy>)\\nImportant\\n💡 [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/user_keys>)\\n```\\nimport openai # openai v1.0.0+\\nclient = openai.OpenAI(api_key=\"anything\",base_url=\"http://0.0.0.0:4000\") # set proxy to base_url\\n# request sent to model set on litellm proxy, `litellm --model`\\nresponse = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"this is a test request, write a short poem\"\\n  }\\n])\\nprint(response)\\n```\\n\\n## Proxy Key Management ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys>))\\n[](https://github.com/BerriAI/<#proxy-key-management-docs>)\\nConnect the proxy with a Postgres DB to create proxy keys\\n```\\n# Get the code\\ngit clone https://github.com/BerriAI/litellm\\n# Go to folder\\ncd litellm\\n# Add the master key - you can change this after setup\\necho \\'LITELLM_MASTER_KEY=\"sk-1234\"\\' > .env\\n# Add the litellm salt key - you cannot change this after adding a model\\n# It is used to encrypt / decrypt your LLM API Key credentials\\n# We recommend - https://1password.com/password-generator/ \\n# password generator to get a random hash for litellm salt key\\necho \\'LITELLM_SALT_KEY=\"sk-1234\"\\' > .env\\nsource .env\\n# Start\\ndocker-compose up\\n```\\n\\nUI on `/ui` on your proxy server [![ui_3](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mzk5Nzk0MDksIm5iZiI6MTczOTk3OTEwOSwicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjE5VDE1MzE0OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU5MWRmMjg3OTEyMTYzYWY5ZjJjMzQ1NDg1MGFiMDEzZmRiYjlhYzUzZmU5YjY1MjI2ODc4ZjQxYmNhYTA4YjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i6LmFjearIk1tr1dFS5--akb541UBpCrINuoukaqrUE)](https://github.com/BerriAI/<https:/private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mzk5Nzk0MDksIm5iZiI6MTczOTk3OTEwOSwicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjE5VDE1MzE0OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU5MWRmMjg3OTEyMTYzYWY5ZjJjMzQ1NDg1MGFiMDEzZmRiYjlhYzUzZmU5YjY1MjI2ODc4ZjQxYmNhYTA4YjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i6LmFjearIk1tr1dFS5--akb541UBpCrINuoukaqrUE>)\\nSet budgets and rate limits across multiple projects `POST /key/generate`\\n### Request\\n[](https://github.com/BerriAI/<#request>)\\n```\\ncurl \\'http://0.0.0.0:4000/key/generate\\' \\\\\\n--header \\'Authorization: Bearer sk-1234\\' \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data-raw \\'{\"models\": [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-2\"], \"duration\": \"20m\",\"metadata\": {\"user\": \"ishaan@berri.ai\", \"team\": \"core-infra\"}}\\'\\n```\\n\\n### Expected Response\\n[](https://github.com/BerriAI/<#expected-response>)\\n```\\n{\\n  \"key\": \"sk-kdEXbIqZRwEeEiHwdg7sFA\", # Bearer token\\n  \"expires\": \"2023-11-19T01:38:25.838000+00:00\" # datetime object\\n}\\n```\\n\\n## Supported Providers ([Docs](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers>))\\n[](https://github.com/BerriAI/<#supported-providers-docs>)\\nProvider | [Completion](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/#basic-usage>) | [Streaming](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#streaming-responses>) | [Async Completion](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#async-completion>) | [Async Streaming](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#async-streaming>) | [Async Embedding](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/embedding/supported_embedding>) | [Async Image Generation](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/image_generation>)  \\n---|---|---|---|---|---|---  \\n[openai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/openai>) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\n[azure](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/azure>) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\n[AI/ML API](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aiml>) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\n[aws - sagemaker](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aws_sagemaker>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[aws - bedrock](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/bedrock>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[google - vertex_ai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/vertex>) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\n[google - palm](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/palm>) | ✅ | ✅ | ✅ | ✅  \\n[google AI Studio - gemini](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/gemini>) | ✅ | ✅ | ✅ | ✅  \\n[mistral ai api](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/mistral>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[cloudflare AI Workers](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/cloudflare_workers>) | ✅ | ✅ | ✅ | ✅  \\n[cohere](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/cohere>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[anthropic](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/anthropic>) | ✅ | ✅ | ✅ | ✅  \\n[empower](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/empower>) | ✅ | ✅ | ✅ | ✅  \\n[huggingface](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/huggingface>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[replicate](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/replicate>) | ✅ | ✅ | ✅ | ✅  \\n[together_ai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/togetherai>) | ✅ | ✅ | ✅ | ✅  \\n[openrouter](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/openrouter>) | ✅ | ✅ | ✅ | ✅  \\n[ai21](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/ai21>) | ✅ | ✅ | ✅ | ✅  \\n[baseten](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/baseten>) | ✅ | ✅ | ✅ | ✅  \\n[vllm](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/vllm>) | ✅ | ✅ | ✅ | ✅  \\n[nlp_cloud](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/nlp_cloud>) | ✅ | ✅ | ✅ | ✅  \\n[aleph alpha](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aleph_alpha>) | ✅ | ✅ | ✅ | ✅  \\n[petals](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/petals>) | ✅ | ✅ | ✅ | ✅  \\n[ollama](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/ollama>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[deepinfra](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/deepinfra>) | ✅ | ✅ | ✅ | ✅  \\n[perplexity-ai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/perplexity>) | ✅ | ✅ | ✅ | ✅  \\n[Groq AI](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/groq>) | ✅ | ✅ | ✅ | ✅  \\n[Deepseek](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/deepseek>) | ✅ | ✅ | ✅ | ✅  \\n[anyscale](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/anyscale>) | ✅ | ✅ | ✅ | ✅  \\n[IBM - watsonx.ai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/watsonx>) | ✅ | ✅ | ✅ | ✅ | ✅  \\n[voyage ai](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/voyage>) | ✅  \\n[xinference [Xorbits Inference]](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/xinference>) | ✅  \\n[FriendliAI](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/friendliai>) | ✅ | ✅ | ✅ | ✅  \\n[Galadriel](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/galadriel>) | ✅ | ✅ | ✅ | ✅  \\n[**Read the Docs**](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/>)\\n## Contributing\\n[](https://github.com/BerriAI/<#contributing>)\\nTo contribute: Clone the repo locally -> Make a change -> Submit a PR with the change.\\nHere\\'s how to modify the repo locally: Step 1: Clone the repo\\n```\\ngit clone https://github.com/BerriAI/litellm.git\\n\\n```\\n\\nStep 2: Navigate into the project, and install dependencies:\\n```\\ncd litellm\\npoetry install -E extra_proxy -E proxy\\n\\n```\\n\\nStep 3: Test your change:\\n```\\ncd tests # pwd: Documents/litellm/litellm/tests\\npoetry run flake8\\npoetry run pytest .\\n\\n```\\n\\nStep 4: Submit a PR with your changes! 🚀\\n  * push your fork to your GitHub repo\\n  * submit a PR from there\\n\\n\\n### Building LiteLLM Docker Image\\n[](https://github.com/BerriAI/<#building-litellm-docker-image>)\\nFollow these instructions if you want to build / run the LiteLLM Docker Image yourself.\\nStep 1: Clone the repo\\n```\\ngit clone https://github.com/BerriAI/litellm.git\\n\\n```\\n\\nStep 2: Build the Docker Image\\nBuild using Dockerfile.non_root\\n```\\ndocker build -f docker/Dockerfile.non_root -t litellm_test_image .\\n\\n```\\n\\nStep 3: Run the Docker Image\\nMake sure config.yaml is present in the root directory. This is your litellm proxy config file.\\n```\\ndocker run \\\\\\n  -v $(pwd)/proxy_config.yaml:/app/config.yaml \\\\\\n  -e DATABASE_URL=\"postgresql://xxxxxxxx\" \\\\\\n  -e LITELLM_MASTER_KEY=\"sk-1234\" \\\\\\n  -p 4000:4000 \\\\\\n  litellm_test_image \\\\\\n  --config /app/config.yaml --detailed_debug\\n\\n```\\n\\n# Enterprise\\n[](https://github.com/BerriAI/<#enterprise>)\\nFor companies that need better security, user management and professional support\\n[Talk to founders](https://github.com/BerriAI/<https:/calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat>)\\nThis covers:\\n  * ✅ **Features under the[LiteLLM Commercial License](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/enterprise>):**\\n  * ✅ **Feature Prioritization**\\n  * ✅ **Custom Integrations**\\n  * ✅ **Professional Support - Dedicated discord + slack**\\n  * ✅ **Custom SLAs**\\n  * ✅ **Secure access with Single Sign-On**\\n\\n\\n# Code Quality / Linting\\n[](https://github.com/BerriAI/<#code-quality--linting>)\\nLiteLLM follows the [Google Python Style Guide](https://github.com/BerriAI/<https:/google.github.io/styleguide/pyguide.html>).\\nWe run:\\n  * Ruff for [formatting and linting checks](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320>)\\n  * Mypy + Pyright for typing [1](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90>), [2](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4>)\\n  * Black for [formatting](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79>)\\n  * isort for [import sorting](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10>)\\n\\n\\nIf you have suggestions on how to improve the code quality feel free to open an issue or a PR.\\n# Support / talk with founders\\n[](https://github.com/BerriAI/<#support--talk-with-founders>)\\n  * [Schedule Demo 👋](https://github.com/BerriAI/<https:/calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version>)\\n  * [Community Discord 💭](https://github.com/BerriAI/<https:/discord.gg/wuPM9dRgDw>)\\n  * Our numbers 📞 +1 (770) 8783-106 / \\u202d+1 (412) 618-6238\\u202c\\n  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai\\n\\n\\n# Why did we build this\\n[](https://github.com/BerriAI/<#why-did-we-build-this>)\\n  * **Need for simplicity** : Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.\\n\\n\\n# Contributors\\n[](https://github.com/BerriAI/<#contributors>)\\n[ ![](https://camo.githubusercontent.com/8e29b23dcec9d07b46521758c401a2f3e4906ffe41e35179bd9908e7c4eeaa2a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d426572726941492f6c6974656c6c6d) ](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/graphs/contributors>)\\n## Run in Developer mode\\n[](https://github.com/BerriAI/<#run-in-developer-mode>)\\n### Services\\n[](https://github.com/BerriAI/<#services>)\\n  1. Setup .env file in root\\n  2. Run dependant services `docker-compose up db prometheus`\\n\\n\\n### Backend\\n[](https://github.com/BerriAI/<#backend>)\\n  1. (In root) create virtual environment `python -m venv .venv`\\n  2. Activate virtual environment `source .venv/bin/activate`\\n  3. Install dependencies `pip install -e \".[all]\"`\\n  4. Start proxy backend `uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload`\\n\\n\\n### Frontend\\n[](https://github.com/BerriAI/<#frontend>)\\n  1. Navigate to `ui/litellm-dashboard`\\n  2. Install dependencies `npm install`\\n  3. Run `npm run dev` to start the dashboard\\n\\n\\n## About\\nPython SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq] \\n[docs.litellm.ai/docs/](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/> \"https://docs.litellm.ai/docs/\")\\n### Topics\\n[ gateway ](https://github.com/BerriAI/</topics/gateway> \"Topic: gateway\") [ bedrock ](https://github.com/BerriAI/</topics/bedrock> \"Topic: bedrock\") [ openai ](https://github.com/BerriAI/</topics/openai> \"Topic: openai\") [ vertex-ai ](https://github.com/BerriAI/</topics/vertex-ai> \"Topic: vertex-ai\") [ azure-openai ](https://github.com/BerriAI/</topics/azure-openai> \"Topic: azure-openai\") [ llm ](https://github.com/BerriAI/</topics/llm> \"Topic: llm\") [ langchain ](https://github.com/BerriAI/</topics/langchain> \"Topic: langchain\") [ llmops ](https://github.com/BerriAI/</topics/llmops> \"Topic: llmops\") [ anthropic ](https://github.com/BerriAI/</topics/anthropic> \"Topic: anthropic\") [ openai-proxy ](https://github.com/BerriAI/</topics/openai-proxy> \"Topic: openai-proxy\") [ ai-gateway ](https://github.com/BerriAI/</topics/ai-gateway> \"Topic: ai-gateway\") [ llm-gateway ](https://github.com/BerriAI/</topics/llm-gateway> \"Topic: llm-gateway\")\\n### Resources\\n[ Readme ](https://github.com/BerriAI/<#readme-ov-file>)\\n### License\\n[ View license ](https://github.com/BerriAI/<#License-1-ov-file>)\\n### Security policy\\n[ Security policy ](https://github.com/BerriAI/<#security-ov-file>)\\n[ Activity](https://github.com/BerriAI/</BerriAI/litellm/activity>)\\n[ Custom properties](https://github.com/BerriAI/</BerriAI/litellm/custom-properties>)\\n### Stars\\n[ **17.7k** stars](https://github.com/BerriAI/</BerriAI/litellm/stargazers>)\\n### Watchers\\n[ **95** watching](https://github.com/BerriAI/</BerriAI/litellm/watchers>)\\n### Forks\\n[ **2.2k** forks](https://github.com/BerriAI/</BerriAI/litellm/forks>)\\n[ Report repository ](https://github.com/BerriAI/</contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm&report=BerriAI+%28user%29>)\\n##  [Releases 764](https://github.com/BerriAI/</BerriAI/litellm/releases>)\\n[ v1.61.9-nightly Latest  Feb 19, 2025 ](https://github.com/BerriAI/</BerriAI/litellm/releases/tag/v1.61.9-nightly>)\\n[+ 763 releases](https://github.com/BerriAI/</BerriAI/litellm/releases>)\\n## Sponsor this project\\n  * <https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS>\\n\\n\\n##  [Packages 0](https://github.com/BerriAI/</orgs/BerriAI/packages?repo_name=litellm>)\\n##  [Used by 7.1k](https://github.com/BerriAI/</BerriAI/litellm/network/dependents>)\\n[\\n  * ![@magicKraken](https://avatars.githubusercontent.com/u/51038655?s=64&v=4)\\n  * ![@jaimin-simform](https://avatars.githubusercontent.com/u/147166467?s=64&v=4)\\n  * ![@Joel-moraes](https://avatars.githubusercontent.com/u/199928601?s=64&v=4)\\n  * ![@JohannesBeckerr](https://avatars.githubusercontent.com/u/198551118?s=64&v=4)\\n  * ![@zwy](https://avatars.githubusercontent.com/u/4981487?s=64&v=4)\\n  * ![@krishshah9944](https://avatars.githubusercontent.com/u/153007529?s=64&v=4)\\n  * ![@TylerDurden120](https://avatars.githubusercontent.com/u/181817643?s=64&v=4)\\n  * ![@Wintoo12](https://avatars.githubusercontent.com/u/164622679?s=64&v=4)\\n\\n+ 7,068  ](https://github.com/BerriAI/</BerriAI/litellm/network/dependents>)\\n##  [Contributors 437](https://github.com/BerriAI/</BerriAI/litellm/graphs/contributors>)\\n[+ 423 contributors](https://github.com/BerriAI/</BerriAI/litellm/graphs/contributors>)\\n## Languages\\n  * [ Python 92.8% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=python>)\\n  * [ TypeScript 6.3% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=typescript>)\\n  * [ HTML 0.7% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=html>)\\n  * [ JavaScript 0.2% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=javascript>)\\n  * [ Shell 0.0% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=shell>)\\n  * [ Dockerfile 0.0% ](https://github.com/BerriAI/</BerriAI/litellm/search?l=dockerfile>)\\n\\n\\n## Footer\\n[ ](https://github.com/BerriAI/<https:/github.com> \"GitHub\") © 2025 GitHub, Inc. \\n### Footer navigation\\n  * [Terms](https://github.com/BerriAI/<https:/docs.github.com/site-policy/github-terms/github-terms-of-service>)\\n  * [Privacy](https://github.com/BerriAI/<https:/docs.github.com/site-policy/privacy-policies/github-privacy-statement>)\\n  * [Security](https://github.com/BerriAI/<https:/github.com/security>)\\n  * [Status](https://github.com/BerriAI/<https:/www.githubstatus.com/>)\\n  * [Docs](https://github.com/BerriAI/<https:/docs.github.com/>)\\n  * [Contact](https://github.com/BerriAI/<https:/support.github.com?tags=dotcom-footer>)\\n  * Manage cookies \\n  * Do not share my personal information \\n\\n\\nYou can’t perform that action at this time. \\n' markdown_with_citations='Skip to content⟨1⟩\\n## Navigation Menu\\nToggle navigation\\n ⟨2⟩\\n Sign in ⟨3⟩\\n  * Product \\n    *  GitHub Copilot Write better code with AI  ⟨4⟩\\n    *  Security Find and fix vulnerabilities  ⟨5⟩\\n    *  Actions Automate any workflow  ⟨6⟩\\n    *  Codespaces Instant dev environments  ⟨7⟩\\n    *  Issues Plan and track work  ⟨8⟩\\n    *  Code Review Manage code changes  ⟨9⟩\\n    *  Discussions Collaborate outside of code  ⟨10⟩\\n    *  Code Search Find more, search less  ⟨11⟩\\nExplore\\n    *  All features ⟨12⟩\\n    *  Documentation ⟨13⟩\\n    *  GitHub Skills ⟨14⟩\\n    *  Blog ⟨15⟩\\n  * Solutions \\nBy company size\\n    *  Enterprises ⟨16⟩\\n    *  Small and medium teams ⟨17⟩\\n    *  Startups ⟨18⟩\\n    *  Nonprofits ⟨19⟩\\nBy use case\\n    *  DevSecOps ⟨20⟩\\n    *  DevOps ⟨21⟩\\n    *  CI/CD ⟨22⟩\\n    *  View all use cases ⟨23⟩\\nBy industry\\n    *  Healthcare ⟨24⟩\\n    *  Financial services ⟨25⟩\\n    *  Manufacturing ⟨26⟩\\n    *  Government ⟨27⟩\\n    *  View all industries ⟨28⟩\\n View all solutions ⟨29⟩\\n  * Resources \\nTopics\\n    *  AI ⟨30⟩\\n    *  DevOps ⟨31⟩\\n    *  Security ⟨32⟩\\n    *  Software Development ⟨33⟩\\n    *  View all ⟨34⟩\\nExplore\\n    *  Learning Pathways ⟨35⟩\\n    *  White papers, Ebooks, Webinars ⟨36⟩\\n    *  Customer Stories ⟨37⟩\\n    *  Partners ⟨38⟩\\n    *  Executive Insights ⟨39⟩\\n  * Open Source \\n    *  GitHub Sponsors Fund open source developers  ⟨40⟩\\n    *  The ReadME Project GitHub community articles  ⟨41⟩\\nRepositories\\n    *  Topics ⟨42⟩\\n    *  Trending ⟨43⟩\\n    *  Collections ⟨44⟩\\n  * Enterprise \\n    *  Enterprise platform AI-powered developer platform  ⟨45⟩\\nAvailable add-ons\\n    *  Advanced Security Enterprise-grade security features  ⟨46⟩\\n    *  GitHub Copilot Enterprise-grade AI features  ⟨47⟩\\n    *  Premium Support Enterprise-grade 24/7 support  ⟨48⟩\\n  * Pricing⟨49⟩\\n\\n\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\nSearch \\nClear\\nSearch syntax tips⟨50⟩\\n#  Provide feedback \\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancel  Submit feedback \\n#  Saved searches \\n## Use saved searches to filter your results more quickly\\nName\\nQuery\\nTo see all available qualifiers, see our documentation⟨50⟩. \\nCancel  Create saved search \\n Sign in ⟨3⟩\\n Sign up ⟨51⟩ Reseting focus\\nYou signed in with another tab or window. Reload⟨52⟩ to refresh your session. You signed out in another tab or window. Reload⟨52⟩ to refresh your session. You switched accounts on another tab or window. Reload⟨52⟩ to refresh your session. Dismiss alert\\n{{ message }}\\n BerriAI ⟨53⟩ / **litellm⟨54⟩ ** Public\\n  *  Notifications ⟨55⟩ You must be signed in to change notification settings\\n  *  Fork 2.2k ⟨55⟩\\n  *  Star  17.7k ⟨55⟩\\n\\n\\nPython SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq] \\ndocs.litellm.ai/docs/⟨56⟩\\n### License\\n View license ⟨57⟩\\n 17.7k stars ⟨58⟩  2.2k forks ⟨59⟩  Branches ⟨60⟩  Tags ⟨61⟩  Activity ⟨62⟩\\n Star  ⟨55⟩\\n Notifications ⟨55⟩ You must be signed in to change notification settings\\n  *  Code ⟨54⟩\\n  *  Issues 948 ⟨63⟩\\n  *  Pull requests 300 ⟨64⟩\\n  *  Discussions ⟨65⟩\\n  *  Actions ⟨66⟩\\n  *  Projects 0 ⟨67⟩\\n  *  Security ⟨68⟩\\n  *  Insights ⟨69⟩\\n\\n\\nAdditional navigation options\\n  *  Code  ⟨54⟩\\n  *  Issues  ⟨63⟩\\n  *  Pull requests  ⟨64⟩\\n  *  Discussions  ⟨65⟩\\n  *  Actions  ⟨66⟩\\n  *  Projects  ⟨67⟩\\n  *  Security  ⟨68⟩\\n  *  Insights  ⟨69⟩\\n\\n\\n# BerriAI/litellm\\nmain\\nBranches⟨60⟩Tags⟨61⟩\\n[](https://github.com/BerriAI/</BerriAI/litellm/branches>)[](https://github.com/BerriAI/</BerriAI/litellm/tags>)\\nGo to file\\nCode\\n## Folders and files\\nName| Name| Last commit message| Last commit date  \\n---|---|---|---  \\n## Latest commit\\n## History\\n19,542 Commits⟨70⟩[](https://github.com/BerriAI/</BerriAI/litellm/commits/main/>)  \\n.circleci⟨71⟩| .circleci⟨71⟩  \\n.devcontainer⟨72⟩| .devcontainer⟨72⟩  \\n.github⟨73⟩| .github⟨73⟩  \\nci_cd⟨74⟩| ci_cd⟨74⟩  \\ncookbook⟨75⟩| cookbook⟨75⟩  \\ndb_scripts⟨76⟩| db_scripts⟨76⟩  \\ndeploy⟨77⟩| deploy⟨77⟩  \\ndist⟨78⟩| dist⟨78⟩  \\ndocker⟨79⟩| docker⟨79⟩  \\ndocs/my-website⟨80⟩| docs/my-website⟨80⟩  \\nenterprise⟨81⟩| enterprise⟨81⟩  \\nlitellm-js⟨82⟩| litellm-js⟨82⟩  \\nlitellm⟨83⟩| litellm⟨83⟩  \\ntests⟨84⟩| tests⟨84⟩  \\nui⟨85⟩| ui⟨85⟩  \\n.dockerignore⟨86⟩| .dockerignore⟨86⟩  \\n.env.example⟨87⟩| .env.example⟨87⟩  \\n.flake8⟨88⟩| .flake8⟨88⟩  \\n.git-blame-ignore-revs⟨89⟩| .git-blame-ignore-revs⟨89⟩  \\n.gitattributes⟨90⟩| .gitattributes⟨90⟩  \\n.gitignore⟨91⟩| .gitignore⟨91⟩  \\n.pre-commit-config.yaml⟨92⟩| .pre-commit-config.yaml⟨92⟩  \\nDockerfile⟨93⟩| Dockerfile⟨93⟩  \\nLICENSE⟨57⟩| LICENSE⟨57⟩  \\nREADME.md⟨94⟩| README.md⟨94⟩  \\ncodecov.yaml⟨95⟩| codecov.yaml⟨95⟩  \\ndocker-compose.yml⟨96⟩| docker-compose.yml⟨96⟩  \\nindex.yaml⟨97⟩| index.yaml⟨97⟩  \\nmodel_prices_and_context_window.json⟨98⟩| model_prices_and_context_window.json⟨98⟩  \\nmypy.ini⟨99⟩| mypy.ini⟨99⟩  \\npackage-lock.json⟨100⟩| package-lock.json⟨100⟩  \\npackage.json⟨101⟩| package.json⟨101⟩  \\npoetry.lock⟨102⟩| poetry.lock⟨102⟩  \\nprometheus.yml⟨103⟩| prometheus.yml⟨103⟩  \\nproxy_server_config.yaml⟨104⟩| proxy_server_config.yaml⟨104⟩  \\npyproject.toml⟨105⟩| pyproject.toml⟨105⟩  \\npyrightconfig.json⟨106⟩| pyrightconfig.json⟨106⟩  \\nrender.yaml⟨107⟩| render.yaml⟨107⟩  \\nrequirements.txt⟨108⟩| requirements.txt⟨108⟩  \\nruff.toml⟨109⟩| ruff.toml⟨109⟩  \\nschema.prisma⟨110⟩| schema.prisma⟨110⟩  \\nsecurity.md⟨111⟩| security.md⟨111⟩  \\nView all files  \\n## Repository files navigation\\n  * README⟨112⟩\\n  * License⟨112⟩\\n  * Security⟨112⟩\\n\\n\\n#  🚅 LiteLLM \\n[](https://github.com/BerriAI/<#---------litellm---->)\\n![Deploy to Render⟨113⟩](https://github.com/BerriAI/<https:/render.com/deploy?repo=https://github.com/BerriAI/litellm>)  ![Deploy on Railway⟨114⟩ ](https://github.com/BerriAI/<https:/railway.app/template/HLP0Ub?referralCode=jch2ME>)\\nCall all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] \\n#### LiteLLM Proxy Server (LLM Gateway)⟨115⟩ |  Hosted Proxy (Preview)⟨116⟩ | Enterprise Tier⟨117⟩\\n[](https://github.com/BerriAI/<#litellm-proxy-server-llm-gateway---hosted-proxy-preview--enterprise-tier>)\\n####   ![PyPI Version⟨118⟩ ](https://github.com/BerriAI/<https:/pypi.org/project/litellm/>)  ![CircleCI⟨119⟩ ](https://github.com/BerriAI/<https:/dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main>)  ![Y Combinator W23⟨120⟩ ](https://github.com/BerriAI/<https:/www.ycombinator.com/companies/berriai>)  ![Whatsapp⟨121⟩ ](https://github.com/BerriAI/<https:/wa.link/huol9n>)  ![Discord⟨122⟩ ](https://github.com/BerriAI/<https:/discord.gg/wuPM9dRgDw>)\\n[](https://github.com/BerriAI/<#-------------------------------------------------------------------------------->)\\nLiteLLM manages:\\n  * Translate inputs to provider\\'s `completion`, `embedding`, and `image_generation` endpoints\\n  * Consistent output⟨123⟩, text responses will always be available at `[\\'choices\\'][0][\\'message\\'][\\'content\\']`\\n  * Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router⟨124⟩\\n  * Set Budgets & Rate limits per project, api key, model LiteLLM Proxy Server (LLM Gateway)⟨115⟩\\n\\n\\n**Jump to LiteLLM Proxy (LLM Gateway) Docs**⟨125⟩ **Jump to Supported LLM Providers**⟨126⟩\\n🚨 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published.\\nSupport for more providers. Missing a provider or LLM Platform, raise a feature request⟨127⟩.\\n# Usage (**Docs**⟨56⟩)\\n[](https://github.com/BerriAI/<#usage-docs>)\\nImportant\\nLiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide here⟨128⟩ LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.\\n ![Open In Colab⟨129⟩ ](https://github.com/BerriAI/<https:/colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb>)\\n```\\npip install litellm\\n```\\n\\n```\\nfrom litellm import completion\\nimport os\\n## set ENV variables\\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\\nmessages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\\n# openai call\\nresponse = completion(model=\"openai/gpt-4o\", messages=messages)\\n# anthropic call\\nresponse = completion(model=\"anthropic/claude-3-sonnet-20240229\", messages=messages)\\nprint(response)\\n```\\n\\n### Response (OpenAI Format)\\n[](https://github.com/BerriAI/<#response-openai-format>)\\n```\\n{\\n  \"id\": \"chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885\",\\n  \"created\": 1734366691,\\n  \"model\": \"claude-3-sonnet-20240229\",\\n  \"object\": \"chat.completion\",\\n  \"system_fingerprint\": null,\\n  \"choices\": [\\n    {\\n      \"finish_reason\": \"stop\",\\n      \"index\": 0,\\n      \"message\": {\\n        \"content\": \"Hello! As an AI language model, I don\\'t have feelings, but I\\'m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\",\\n        \"role\": \"assistant\",\\n        \"tool_calls\": null,\\n        \"function_call\": null\\n      }\\n    }\\n  ],\\n  \"usage\": {\\n    \"completion_tokens\": 43,\\n    \"prompt_tokens\": 13,\\n    \"total_tokens\": 56,\\n    \"completion_tokens_details\": null,\\n    \"prompt_tokens_details\": {\\n      \"audio_tokens\": null,\\n      \"cached_tokens\": 0\\n    },\\n    \"cache_creation_input_tokens\": 0,\\n    \"cache_read_input_tokens\": 0\\n  }\\n}\\n```\\n\\nCall any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to provider docs for more information⟨130⟩\\n## Async (Docs⟨131⟩)\\n[](https://github.com/BerriAI/<#async-docs>)\\n```\\nfrom litellm import acompletion\\nimport asyncio\\nasync def test_get_response():\\n  user_message = \"Hello, how are you?\"\\n  messages = [{\"content\": user_message, \"role\": \"user\"}]\\n  response = await acompletion(model=\"openai/gpt-4o\", messages=messages)\\n  return response\\nresponse = asyncio.run(test_get_response())\\nprint(response)\\n```\\n\\n## Streaming (Docs⟨132⟩)\\n[](https://github.com/BerriAI/<#streaming-docs>)\\nliteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response. Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)\\n```\\nfrom litellm import completion\\nresponse = completion(model=\"openai/gpt-4o\", messages=messages, stream=True)\\nfor part in response:\\n  print(part.choices[0].delta.content or \"\")\\n# claude 2\\nresponse = completion(\\'anthropic/claude-3-sonnet-20240229\\', messages, stream=True)\\nfor part in response:\\n  print(part)\\n```\\n\\n### Response chunk (OpenAI Format)\\n[](https://github.com/BerriAI/<#response-chunk-openai-format>)\\n```\\n{\\n  \"id\": \"chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697\",\\n  \"created\": 1734366925,\\n  \"model\": \"claude-3-sonnet-20240229\",\\n  \"object\": \"chat.completion.chunk\",\\n  \"system_fingerprint\": null,\\n  \"choices\": [\\n    {\\n      \"finish_reason\": null,\\n      \"index\": 0,\\n      \"delta\": {\\n        \"content\": \"Hello\",\\n        \"role\": \"assistant\",\\n        \"function_call\": null,\\n        \"tool_calls\": null,\\n        \"audio\": null\\n      },\\n      \"logprobs\": null\\n    }\\n  ]\\n}\\n```\\n\\n## Logging Observability (Docs⟨133⟩)\\n[](https://github.com/BerriAI/<#logging-observability-docs>)\\nLiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack\\n```\\nfrom litellm import completion\\n## set env variables for logging tools (when using MLflow, no API key set up is required)\\nos.environ[\"LUNARY_PUBLIC_KEY\"] = \"your-lunary-public-key\"\\nos.environ[\"HELICONE_API_KEY\"] = \"your-helicone-auth-key\"\\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\\nos.environ[\"ATHINA_API_KEY\"] = \"your-athina-api-key\"\\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\\n# set callbacks\\nlitellm.success_callback = [\"lunary\", \"mlflow\", \"langfuse\", \"athina\", \"helicone\"] # log input/output to lunary, langfuse, supabase, athina, helicone etc\\n#openai call\\nresponse = completion(model=\"openai/gpt-4o\", messages=[{\"role\": \"user\", \"content\": \"Hi 👋 - i\\'m openai\"}])\\n```\\n\\n# LiteLLM Proxy Server (LLM Gateway) - (Docs⟨115⟩)\\n[](https://github.com/BerriAI/<#litellm-proxy-server-llm-gateway---docs>)\\nTrack spend + Load Balance across multiple projects\\nHosted Proxy (Preview)⟨116⟩\\nThe proxy provides:\\n  1. Hooks for auth⟨134⟩\\n  2. Hooks for logging⟨135⟩\\n  3. Cost tracking⟨136⟩\\n  4. Rate Limiting⟨137⟩\\n\\n\\n## 📖 Proxy Endpoints - Swagger Docs⟨138⟩\\n[](https://github.com/BerriAI/<#-proxy-endpoints---swagger-docs>)\\n## Quick Start Proxy - CLI\\n[](https://github.com/BerriAI/<#quick-start-proxy---cli>)\\n```\\npip install \\'litellm[proxy]\\'\\n```\\n\\n### Step 1: Start litellm proxy\\n[](https://github.com/BerriAI/<#step-1-start-litellm-proxy>)\\n```\\n$ litellm --model huggingface/bigcode/starcoder\\n#INFO: Proxy running on http://0.0.0.0:4000\\n```\\n\\n### Step 2: Make ChatCompletions Request to Proxy\\n[](https://github.com/BerriAI/<#step-2-make-chatcompletions-request-to-proxy>)\\nImportant\\n💡 Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl⟨139⟩\\n```\\nimport openai # openai v1.0.0+\\nclient = openai.OpenAI(api_key=\"anything\",base_url=\"http://0.0.0.0:4000\") # set proxy to base_url\\n# request sent to model set on litellm proxy, `litellm --model`\\nresponse = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"this is a test request, write a short poem\"\\n  }\\n])\\nprint(response)\\n```\\n\\n## Proxy Key Management (Docs⟨140⟩)\\n[](https://github.com/BerriAI/<#proxy-key-management-docs>)\\nConnect the proxy with a Postgres DB to create proxy keys\\n```\\n# Get the code\\ngit clone https://github.com/BerriAI/litellm\\n# Go to folder\\ncd litellm\\n# Add the master key - you can change this after setup\\necho \\'LITELLM_MASTER_KEY=\"sk-1234\"\\' > .env\\n# Add the litellm salt key - you cannot change this after adding a model\\n# It is used to encrypt / decrypt your LLM API Key credentials\\n# We recommend - https://1password.com/password-generator/ \\n# password generator to get a random hash for litellm salt key\\necho \\'LITELLM_SALT_KEY=\"sk-1234\"\\' > .env\\nsource .env\\n# Start\\ndocker-compose up\\n```\\n\\nUI on `/ui` on your proxy server ![ui_3⟨141⟩](https://github.com/BerriAI/<https:/private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mzk5Nzk0MDksIm5iZiI6MTczOTk3OTEwOSwicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjE5VDE1MzE0OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU5MWRmMjg3OTEyMTYzYWY5ZjJjMzQ1NDg1MGFiMDEzZmRiYjlhYzUzZmU5YjY1MjI2ODc4ZjQxYmNhYTA4YjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i6LmFjearIk1tr1dFS5--akb541UBpCrINuoukaqrUE>)\\nSet budgets and rate limits across multiple projects `POST /key/generate`\\n### Request\\n[](https://github.com/BerriAI/<#request>)\\n```\\ncurl \\'http://0.0.0.0:4000/key/generate\\' \\\\\\n--header \\'Authorization: Bearer sk-1234\\' \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data-raw \\'{\"models\": [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-2\"], \"duration\": \"20m\",\"metadata\": {\"user\": \"ishaan@berri.ai\", \"team\": \"core-infra\"}}\\'\\n```\\n\\n### Expected Response\\n[](https://github.com/BerriAI/<#expected-response>)\\n```\\n{\\n  \"key\": \"sk-kdEXbIqZRwEeEiHwdg7sFA\", # Bearer token\\n  \"expires\": \"2023-11-19T01:38:25.838000+00:00\" # datetime object\\n}\\n```\\n\\n## Supported Providers (Docs⟨130⟩)\\n[](https://github.com/BerriAI/<#supported-providers-docs>)\\nProvider | Completion⟨142⟩ | Streaming⟨143⟩ | Async Completion⟨131⟩ | Async Streaming⟨144⟩ | Async Embedding⟨145⟩ | Async Image Generation⟨146⟩  \\n---|---|---|---|---|---|---  \\nopenai⟨147⟩ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\nazure⟨148⟩ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\nAI/ML API⟨149⟩ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\naws - sagemaker⟨150⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\naws - bedrock⟨151⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\ngoogle - vertex_ai⟨152⟩ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅  \\ngoogle - palm⟨153⟩ | ✅ | ✅ | ✅ | ✅  \\ngoogle AI Studio - gemini⟨154⟩ | ✅ | ✅ | ✅ | ✅  \\nmistral ai api⟨155⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\ncloudflare AI Workers⟨156⟩ | ✅ | ✅ | ✅ | ✅  \\ncohere⟨157⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\nanthropic⟨158⟩ | ✅ | ✅ | ✅ | ✅  \\nempower⟨159⟩ | ✅ | ✅ | ✅ | ✅  \\nhuggingface⟨160⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\nreplicate⟨161⟩ | ✅ | ✅ | ✅ | ✅  \\ntogether_ai⟨162⟩ | ✅ | ✅ | ✅ | ✅  \\nopenrouter⟨163⟩ | ✅ | ✅ | ✅ | ✅  \\nai21⟨164⟩ | ✅ | ✅ | ✅ | ✅  \\nbaseten⟨165⟩ | ✅ | ✅ | ✅ | ✅  \\nvllm⟨166⟩ | ✅ | ✅ | ✅ | ✅  \\nnlp_cloud⟨167⟩ | ✅ | ✅ | ✅ | ✅  \\naleph alpha⟨168⟩ | ✅ | ✅ | ✅ | ✅  \\npetals⟨169⟩ | ✅ | ✅ | ✅ | ✅  \\nollama⟨170⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\ndeepinfra⟨171⟩ | ✅ | ✅ | ✅ | ✅  \\nperplexity-ai⟨172⟩ | ✅ | ✅ | ✅ | ✅  \\nGroq AI⟨173⟩ | ✅ | ✅ | ✅ | ✅  \\nDeepseek⟨174⟩ | ✅ | ✅ | ✅ | ✅  \\nanyscale⟨175⟩ | ✅ | ✅ | ✅ | ✅  \\nIBM - watsonx.ai⟨176⟩ | ✅ | ✅ | ✅ | ✅ | ✅  \\nvoyage ai⟨177⟩ | ✅  \\n[xinference [Xorbits Inference]](https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/xinference>) | ✅  \\nFriendliAI⟨178⟩ | ✅ | ✅ | ✅ | ✅  \\nGaladriel⟨179⟩ | ✅ | ✅ | ✅ | ✅  \\n**Read the Docs**⟨56⟩\\n## Contributing\\n[](https://github.com/BerriAI/<#contributing>)\\nTo contribute: Clone the repo locally -> Make a change -> Submit a PR with the change.\\nHere\\'s how to modify the repo locally: Step 1: Clone the repo\\n```\\ngit clone https://github.com/BerriAI/litellm.git\\n\\n```\\n\\nStep 2: Navigate into the project, and install dependencies:\\n```\\ncd litellm\\npoetry install -E extra_proxy -E proxy\\n\\n```\\n\\nStep 3: Test your change:\\n```\\ncd tests # pwd: Documents/litellm/litellm/tests\\npoetry run flake8\\npoetry run pytest .\\n\\n```\\n\\nStep 4: Submit a PR with your changes! 🚀\\n  * push your fork to your GitHub repo\\n  * submit a PR from there\\n\\n\\n### Building LiteLLM Docker Image\\n[](https://github.com/BerriAI/<#building-litellm-docker-image>)\\nFollow these instructions if you want to build / run the LiteLLM Docker Image yourself.\\nStep 1: Clone the repo\\n```\\ngit clone https://github.com/BerriAI/litellm.git\\n\\n```\\n\\nStep 2: Build the Docker Image\\nBuild using Dockerfile.non_root\\n```\\ndocker build -f docker/Dockerfile.non_root -t litellm_test_image .\\n\\n```\\n\\nStep 3: Run the Docker Image\\nMake sure config.yaml is present in the root directory. This is your litellm proxy config file.\\n```\\ndocker run \\\\\\n  -v $(pwd)/proxy_config.yaml:/app/config.yaml \\\\\\n  -e DATABASE_URL=\"postgresql://xxxxxxxx\" \\\\\\n  -e LITELLM_MASTER_KEY=\"sk-1234\" \\\\\\n  -p 4000:4000 \\\\\\n  litellm_test_image \\\\\\n  --config /app/config.yaml --detailed_debug\\n\\n```\\n\\n# Enterprise\\n[](https://github.com/BerriAI/<#enterprise>)\\nFor companies that need better security, user management and professional support\\nTalk to founders⟨180⟩\\nThis covers:\\n  * ✅ **Features under theLiteLLM Commercial License⟨181⟩:**\\n  * ✅ **Feature Prioritization**\\n  * ✅ **Custom Integrations**\\n  * ✅ **Professional Support - Dedicated discord + slack**\\n  * ✅ **Custom SLAs**\\n  * ✅ **Secure access with Single Sign-On**\\n\\n\\n# Code Quality / Linting\\n[](https://github.com/BerriAI/<#code-quality--linting>)\\nLiteLLM follows the Google Python Style Guide⟨182⟩.\\nWe run:\\n  * Ruff for formatting and linting checks⟨183⟩\\n  * Mypy + Pyright for typing 1⟨184⟩, 2⟨185⟩\\n  * Black for formatting⟨186⟩\\n  * isort for import sorting⟨187⟩\\n\\n\\nIf you have suggestions on how to improve the code quality feel free to open an issue or a PR.\\n# Support / talk with founders\\n[](https://github.com/BerriAI/<#support--talk-with-founders>)\\n  * Schedule Demo 👋⟨188⟩\\n  * Community Discord 💭⟨189⟩\\n  * Our numbers 📞 +1 (770) 8783-106 / \\u202d+1 (412) 618-6238\\u202c\\n  * Our emails ✉️ ishaan@berri.ai / krrish@berri.ai\\n\\n\\n# Why did we build this\\n[](https://github.com/BerriAI/<#why-did-we-build-this>)\\n  * **Need for simplicity** : Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.\\n\\n\\n# Contributors\\n[](https://github.com/BerriAI/<#contributors>)\\n ![⟨190⟩ ](https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/graphs/contributors>)\\n## Run in Developer mode\\n[](https://github.com/BerriAI/<#run-in-developer-mode>)\\n### Services\\n[](https://github.com/BerriAI/<#services>)\\n  1. Setup .env file in root\\n  2. Run dependant services `docker-compose up db prometheus`\\n\\n\\n### Backend\\n[](https://github.com/BerriAI/<#backend>)\\n  1. (In root) create virtual environment `python -m venv .venv`\\n  2. Activate virtual environment `source .venv/bin/activate`\\n  3. Install dependencies `pip install -e \".[all]\"`\\n  4. Start proxy backend `uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload`\\n\\n\\n### Frontend\\n[](https://github.com/BerriAI/<#frontend>)\\n  1. Navigate to `ui/litellm-dashboard`\\n  2. Install dependencies `npm install`\\n  3. Run `npm run dev` to start the dashboard\\n\\n\\n## About\\nPython SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq] \\ndocs.litellm.ai/docs/⟨56⟩\\n### Topics\\n gateway ⟨191⟩  bedrock ⟨192⟩  openai ⟨193⟩  vertex-ai ⟨194⟩  azure-openai ⟨195⟩  llm ⟨196⟩  langchain ⟨197⟩  llmops ⟨198⟩  anthropic ⟨199⟩  openai-proxy ⟨200⟩  ai-gateway ⟨201⟩  llm-gateway ⟨202⟩\\n### Resources\\n Readme ⟨203⟩\\n### License\\n View license ⟨204⟩\\n### Security policy\\n Security policy ⟨205⟩\\n Activity⟨62⟩\\n Custom properties⟨206⟩\\n### Stars\\n **17.7k** stars⟨58⟩\\n### Watchers\\n **95** watching⟨207⟩\\n### Forks\\n **2.2k** forks⟨59⟩\\n Report repository ⟨208⟩\\n##  Releases 764⟨209⟩\\n v1.61.9-nightly Latest  Feb 19, 2025 ⟨210⟩\\n+ 763 releases⟨209⟩\\n## Sponsor this project\\n  * <https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS>\\n\\n\\n##  Packages 0⟨211⟩\\n##  Used by 7.1k⟨212⟩\\n\\n  * ![@magicKraken⟨213⟩\\n  * ![@jaimin-simform⟨214⟩]\\n  * ![@Joel-moraes⟨215⟩]\\n  * ![@JohannesBeckerr⟨216⟩]\\n  * ![@zwy⟨217⟩]\\n  * ![@krishshah9944⟨218⟩]\\n  * ![@TylerDurden120⟨219⟩]\\n  * ![@Wintoo12⟨220⟩]\\n\\n+ 7,068  ](https://github.com/BerriAI/</BerriAI/litellm/network/dependents>)\\n##  Contributors 437⟨221⟩\\n+ 423 contributors⟨221⟩\\n## Languages\\n  *  Python 92.8% ⟨222⟩\\n  *  TypeScript 6.3% ⟨223⟩\\n  *  HTML 0.7% ⟨224⟩\\n  *  JavaScript 0.2% ⟨225⟩\\n  *  Shell 0.0% ⟨226⟩\\n  *  Dockerfile 0.0% ⟨227⟩\\n\\n\\n## Footer\\n ⟨228⟩ © 2025 GitHub, Inc. \\n### Footer navigation\\n  * Terms⟨229⟩\\n  * Privacy⟨230⟩\\n  * Security⟨231⟩\\n  * Status⟨232⟩\\n  * Docs⟨233⟩\\n  * Contact⟨234⟩\\n  * Manage cookies \\n  * Do not share my personal information \\n\\n\\nYou can’t perform that action at this time. \\n' references_markdown='\\n\\n## References\\n\\n⟨1⟩ https://github.com/BerriAI/<#start-of-content>: Skip to content\\n⟨2⟩ https://github.com/BerriAI/</>:  \\n⟨3⟩ https://github.com/BerriAI/</login?return_to=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm>:  Sign in \\n⟨4⟩ https://github.com/BerriAI/<https:/github.com/features/copilot>:  GitHub Copilot Write better code with AI  \\n⟨5⟩ https://github.com/BerriAI/<https:/github.com/features/security>:  Security Find and fix vulnerabilities  \\n⟨6⟩ https://github.com/BerriAI/<https:/github.com/features/actions>:  Actions Automate any workflow  \\n⟨7⟩ https://github.com/BerriAI/<https:/github.com/features/codespaces>:  Codespaces Instant dev environments  \\n⟨8⟩ https://github.com/BerriAI/<https:/github.com/features/issues>:  Issues Plan and track work  \\n⟨9⟩ https://github.com/BerriAI/<https:/github.com/features/code-review>:  Code Review Manage code changes  \\n⟨10⟩ https://github.com/BerriAI/<https:/github.com/features/discussions>:  Discussions Collaborate outside of code  \\n⟨11⟩ https://github.com/BerriAI/<https:/github.com/features/code-search>:  Code Search Find more, search less  \\n⟨12⟩ https://github.com/BerriAI/<https:/github.com/features>:  All features \\n⟨13⟩ https://github.com/BerriAI/<https:/docs.github.com>:  Documentation \\n⟨14⟩ https://github.com/BerriAI/<https:/skills.github.com>:  GitHub Skills \\n⟨15⟩ https://github.com/BerriAI/<https:/github.blog>:  Blog \\n⟨16⟩ https://github.com/BerriAI/<https:/github.com/enterprise>:  Enterprises \\n⟨17⟩ https://github.com/BerriAI/<https:/github.com/team>:  Small and medium teams \\n⟨18⟩ https://github.com/BerriAI/<https:/github.com/enterprise/startups>:  Startups \\n⟨19⟩ https://github.com/BerriAI/</solutions/industry/nonprofits>:  Nonprofits \\n⟨20⟩ https://github.com/BerriAI/</solutions/use-case/devsecops>:  DevSecOps \\n⟨21⟩ https://github.com/BerriAI/</solutions/use-case/devops>:  DevOps \\n⟨22⟩ https://github.com/BerriAI/</solutions/use-case/ci-cd>:  CI/CD \\n⟨23⟩ https://github.com/BerriAI/</solutions/use-case>:  View all use cases \\n⟨24⟩ https://github.com/BerriAI/</solutions/industry/healthcare>:  Healthcare \\n⟨25⟩ https://github.com/BerriAI/</solutions/industry/financial-services>:  Financial services \\n⟨26⟩ https://github.com/BerriAI/</solutions/industry/manufacturing>:  Manufacturing \\n⟨27⟩ https://github.com/BerriAI/</solutions/industry/government>:  Government \\n⟨28⟩ https://github.com/BerriAI/</solutions/industry>:  View all industries \\n⟨29⟩ https://github.com/BerriAI/</solutions>:  View all solutions \\n⟨30⟩ https://github.com/BerriAI/</resources/articles/ai>:  AI \\n⟨31⟩ https://github.com/BerriAI/</resources/articles/devops>:  DevOps \\n⟨32⟩ https://github.com/BerriAI/</resources/articles/security>:  Security \\n⟨33⟩ https://github.com/BerriAI/</resources/articles/software-development>:  Software Development \\n⟨34⟩ https://github.com/BerriAI/</resources/articles>:  View all \\n⟨35⟩ https://github.com/BerriAI/<https:/resources.github.com/learn/pathways>:  Learning Pathways \\n⟨36⟩ https://github.com/BerriAI/<https:/resources.github.com>:  White papers, Ebooks, Webinars \\n⟨37⟩ https://github.com/BerriAI/<https:/github.com/customer-stories>:  Customer Stories \\n⟨38⟩ https://github.com/BerriAI/<https:/partner.github.com>:  Partners \\n⟨39⟩ https://github.com/BerriAI/<https:/github.com/solutions/executive-insights>:  Executive Insights \\n⟨40⟩ https://github.com/BerriAI/</sponsors>:  GitHub Sponsors Fund open source developers  \\n⟨41⟩ https://github.com/BerriAI/<https:/github.com/readme>:  The ReadME Project GitHub community articles  \\n⟨42⟩ https://github.com/BerriAI/<https:/github.com/topics>:  Topics \\n⟨43⟩ https://github.com/BerriAI/<https:/github.com/trending>:  Trending \\n⟨44⟩ https://github.com/BerriAI/<https:/github.com/collections>:  Collections \\n⟨45⟩ https://github.com/BerriAI/</enterprise>:  Enterprise platform AI-powered developer platform  \\n⟨46⟩ https://github.com/BerriAI/<https:/github.com/enterprise/advanced-security>:  Advanced Security Enterprise-grade security features  \\n⟨47⟩ https://github.com/BerriAI/</features/copilot#enterprise>:  GitHub Copilot Enterprise-grade AI features  \\n⟨48⟩ https://github.com/BerriAI/</premium-support>:  Premium Support Enterprise-grade 24/7 support  \\n⟨49⟩ https://github.com/BerriAI/<https:/github.com/pricing>: Pricing\\n⟨50⟩ https://github.com/BerriAI/<https:/docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax>: Search syntax tips\\n⟨51⟩ https://github.com/BerriAI/</signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=BerriAI%2Flitellm>:  Sign up \\n⟨52⟩ https://github.com/BerriAI/<>: Reload\\n⟨53⟩ https://github.com/BerriAI/</BerriAI>:  BerriAI \\n⟨54⟩ https://github.com/BerriAI/</BerriAI/litellm>: litellm\\n⟨55⟩ https://github.com/BerriAI/</login?return_to=%2FBerriAI%2Flitellm>:  Notifications \\n⟨56⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/>: https://docs.litellm.ai/docs/ - docs.litellm.ai/docs/\\n⟨57⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/LICENSE>:  View license \\n⟨58⟩ https://github.com/BerriAI/</BerriAI/litellm/stargazers>:  17.7k stars \\n⟨59⟩ https://github.com/BerriAI/</BerriAI/litellm/forks>:  2.2k forks \\n⟨60⟩ https://github.com/BerriAI/</BerriAI/litellm/branches>:  Branches \\n⟨61⟩ https://github.com/BerriAI/</BerriAI/litellm/tags>:  Tags \\n⟨62⟩ https://github.com/BerriAI/</BerriAI/litellm/activity>:  Activity \\n⟨63⟩ https://github.com/BerriAI/</BerriAI/litellm/issues>:  Issues 948 \\n⟨64⟩ https://github.com/BerriAI/</BerriAI/litellm/pulls>:  Pull requests 300 \\n⟨65⟩ https://github.com/BerriAI/</BerriAI/litellm/discussions>:  Discussions \\n⟨66⟩ https://github.com/BerriAI/</BerriAI/litellm/actions>:  Actions \\n⟨67⟩ https://github.com/BerriAI/</BerriAI/litellm/projects>:  Projects 0 \\n⟨68⟩ https://github.com/BerriAI/</BerriAI/litellm/security>:  Security \\n⟨69⟩ https://github.com/BerriAI/</BerriAI/litellm/pulse>:  Insights \\n⟨70⟩ https://github.com/BerriAI/</BerriAI/litellm/commits/main/>: 19,542 Commits\\n⟨71⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/.circleci>: .circleci\\n⟨72⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/.devcontainer>: .devcontainer\\n⟨73⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/.github>: .github\\n⟨74⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/ci_cd>: ci_cd\\n⟨75⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/cookbook>: cookbook\\n⟨76⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/db_scripts>: db_scripts\\n⟨77⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/deploy>: deploy\\n⟨78⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/dist>: dist\\n⟨79⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/docker>: docker\\n⟨80⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/docs/my-website>: This path skips through empty directories - docs/my-website\\n⟨81⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/enterprise>: enterprise\\n⟨82⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm-js>: litellm-js\\n⟨83⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/litellm>: litellm\\n⟨84⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/tests>: tests\\n⟨85⟩ https://github.com/BerriAI/</BerriAI/litellm/tree/main/ui>: ui\\n⟨86⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.dockerignore>: .dockerignore\\n⟨87⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.env.example>: .env.example\\n⟨88⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.flake8>: .flake8\\n⟨89⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.git-blame-ignore-revs>: .git-blame-ignore-revs\\n⟨90⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitattributes>: .gitattributes\\n⟨91⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.gitignore>: .gitignore\\n⟨92⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/.pre-commit-config.yaml>: .pre-commit-config.yaml\\n⟨93⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/Dockerfile>: Dockerfile\\n⟨94⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/README.md>: README.md\\n⟨95⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/codecov.yaml>: codecov.yaml\\n⟨96⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/docker-compose.yml>: docker-compose.yml\\n⟨97⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/index.yaml>: index.yaml\\n⟨98⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/model_prices_and_context_window.json>: model_prices_and_context_window.json\\n⟨99⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/mypy.ini>: mypy.ini\\n⟨100⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/package-lock.json>: package-lock.json\\n⟨101⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/package.json>: package.json\\n⟨102⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/poetry.lock>: poetry.lock\\n⟨103⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/prometheus.yml>: prometheus.yml\\n⟨104⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/proxy_server_config.yaml>: proxy_server_config.yaml\\n⟨105⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyproject.toml>: pyproject.toml\\n⟨106⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/pyrightconfig.json>: pyrightconfig.json\\n⟨107⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/render.yaml>: render.yaml\\n⟨108⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/requirements.txt>: requirements.txt\\n⟨109⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/ruff.toml>: ruff.toml\\n⟨110⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/schema.prisma>: schema.prisma\\n⟨111⟩ https://github.com/BerriAI/</BerriAI/litellm/blob/main/security.md>: security.md\\n⟨112⟩ https://github.com/BerriAI/<#>: README\\n⟨113⟩ https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667: ![Deploy to Render\\n⟨114⟩ https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667:  ![Deploy on Railway\\n⟨115⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/simple_proxy>: LiteLLM Proxy Server (LLM Gateway)\\n⟨116⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/hosted>:  Hosted Proxy (Preview)\\n⟨117⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/enterprise>: Enterprise Tier\\n⟨118⟩ https://camo.githubusercontent.com/de190803172c4d35f85e73a0f4eec265b5029bb0ad250f402aac9ca1bd73bd79/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6974656c6c6d2e737667:  ![PyPI Version\\n⟨119⟩ https://camo.githubusercontent.com/ef06f9362d95c52b7b1dc33f5ff1817c1575fa6e9881847927bccb92c6e063e8/68747470733a2f2f646c2e636972636c6563692e636f6d2f7374617475732d62616467652f696d672f67682f426572726941492f6c6974656c6c6d2f747265652f6d61696e2e7376673f7374796c653d737667:  ![CircleCI\\n⟨120⟩ https://camo.githubusercontent.com/e1e0029e353d103690da84a20e88b7051eebbcdede2a1b35d9d1b78b0b0295cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732332d6f72616e67653f7374796c653d666c61742d737175617265:  ![Y Combinator W23\\n⟨121⟩ https://camo.githubusercontent.com/78382e0d13839fedd81996b3e7cbecea33222e5ea36d54d07455a93dfd68e5d7/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d576861747341707026636f6c6f723d73756363657373266c6f676f3d5768617473417070267374796c653d666c61742d737175617265:  ![Whatsapp\\n⟨122⟩ https://camo.githubusercontent.com/bcba2d72b7345e8de3adc1f330b340b72f37842dd275a91c4f31154e23cc8cd0/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d446973636f726426636f6c6f723d626c7565266c6f676f3d446973636f7264267374796c653d666c61742d737175617265:  ![Discord\\n⟨123⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/output>: Consistent output\\n⟨124⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/routing>: Router\\n⟨125⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs>: **Jump to LiteLLM Proxy (LLM Gateway) Docs**\\n⟨126⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs>: **Jump to Supported LLM Providers**\\n⟨127⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+>: feature request\\n⟨128⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/migration>: here\\n⟨129⟩ https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667:  ![Open In Colab\\n⟨130⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers>: provider docs for more information\\n⟨131⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#async-completion>: Docs\\n⟨132⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream>: Docs\\n⟨133⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/observability/callbacks>: Docs\\n⟨134⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys#custom-auth>: Hooks for auth\\n⟨135⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class>: Hooks for logging\\n⟨136⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend>: Cost tracking\\n⟨137⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/users#set-rate-limits>: Rate Limiting\\n⟨138⟩ https://github.com/BerriAI/<https:/litellm-api.up.railway.app/>: Swagger Docs\\n⟨139⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/user_keys>: Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl\\n⟨140⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/virtual_keys>: Docs\\n⟨141⟩ https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mzk5Nzk0MDksIm5iZiI6MTczOTk3OTEwOSwicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjE5VDE1MzE0OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU5MWRmMjg3OTEyMTYzYWY5ZjJjMzQ1NDg1MGFiMDEzZmRiYjlhYzUzZmU5YjY1MjI2ODc4ZjQxYmNhYTA4YjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.i6LmFjearIk1tr1dFS5--akb541UBpCrINuoukaqrUE: ![ui_3\\n⟨142⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/#basic-usage>: Completion\\n⟨143⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#streaming-responses>: Streaming\\n⟨144⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/completion/stream#async-streaming>: Async Streaming\\n⟨145⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/embedding/supported_embedding>: Async Embedding\\n⟨146⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/image_generation>: Async Image Generation\\n⟨147⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/openai>: openai\\n⟨148⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/azure>: azure\\n⟨149⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aiml>: AI/ML API\\n⟨150⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aws_sagemaker>: aws - sagemaker\\n⟨151⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/bedrock>: aws - bedrock\\n⟨152⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/vertex>: google - vertex_ai\\n⟨153⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/palm>: google - palm\\n⟨154⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/gemini>: google AI Studio - gemini\\n⟨155⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/mistral>: mistral ai api\\n⟨156⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/cloudflare_workers>: cloudflare AI Workers\\n⟨157⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/cohere>: cohere\\n⟨158⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/anthropic>: anthropic\\n⟨159⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/empower>: empower\\n⟨160⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/huggingface>: huggingface\\n⟨161⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/replicate>: replicate\\n⟨162⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/togetherai>: together_ai\\n⟨163⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/openrouter>: openrouter\\n⟨164⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/ai21>: ai21\\n⟨165⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/baseten>: baseten\\n⟨166⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/vllm>: vllm\\n⟨167⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/nlp_cloud>: nlp_cloud\\n⟨168⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/aleph_alpha>: aleph alpha\\n⟨169⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/petals>: petals\\n⟨170⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/ollama>: ollama\\n⟨171⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/deepinfra>: deepinfra\\n⟨172⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/perplexity>: perplexity-ai\\n⟨173⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/groq>: Groq AI\\n⟨174⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/deepseek>: Deepseek\\n⟨175⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/anyscale>: anyscale\\n⟨176⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/watsonx>: IBM - watsonx.ai\\n⟨177⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/voyage>: voyage ai\\n⟨178⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/friendliai>: FriendliAI\\n⟨179⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/providers/galadriel>: Galadriel\\n⟨180⟩ https://github.com/BerriAI/<https:/calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat>: Talk to founders\\n⟨181⟩ https://github.com/BerriAI/<https:/docs.litellm.ai/docs/proxy/enterprise>: LiteLLM Commercial License\\n⟨182⟩ https://github.com/BerriAI/<https:/google.github.io/styleguide/pyguide.html>: Google Python Style Guide\\n⟨183⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320>: formatting and linting checks\\n⟨184⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90>: 1\\n⟨185⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4>: 2\\n⟨186⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79>: formatting\\n⟨187⟩ https://github.com/BerriAI/<https:/github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10>: import sorting\\n⟨188⟩ https://github.com/BerriAI/<https:/calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version>: Schedule Demo 👋\\n⟨189⟩ https://github.com/BerriAI/<https:/discord.gg/wuPM9dRgDw>: Community Discord 💭\\n⟨190⟩ https://camo.githubusercontent.com/8e29b23dcec9d07b46521758c401a2f3e4906ffe41e35179bd9908e7c4eeaa2a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d426572726941492f6c6974656c6c6d:  ![\\n⟨191⟩ https://github.com/BerriAI/</topics/gateway>: Topic: gateway -  gateway \\n⟨192⟩ https://github.com/BerriAI/</topics/bedrock>: Topic: bedrock -  bedrock \\n⟨193⟩ https://github.com/BerriAI/</topics/openai>: Topic: openai -  openai \\n⟨194⟩ https://github.com/BerriAI/</topics/vertex-ai>: Topic: vertex-ai -  vertex-ai \\n⟨195⟩ https://github.com/BerriAI/</topics/azure-openai>: Topic: azure-openai -  azure-openai \\n⟨196⟩ https://github.com/BerriAI/</topics/llm>: Topic: llm -  llm \\n⟨197⟩ https://github.com/BerriAI/</topics/langchain>: Topic: langchain -  langchain \\n⟨198⟩ https://github.com/BerriAI/</topics/llmops>: Topic: llmops -  llmops \\n⟨199⟩ https://github.com/BerriAI/</topics/anthropic>: Topic: anthropic -  anthropic \\n⟨200⟩ https://github.com/BerriAI/</topics/openai-proxy>: Topic: openai-proxy -  openai-proxy \\n⟨201⟩ https://github.com/BerriAI/</topics/ai-gateway>: Topic: ai-gateway -  ai-gateway \\n⟨202⟩ https://github.com/BerriAI/</topics/llm-gateway>: Topic: llm-gateway -  llm-gateway \\n⟨203⟩ https://github.com/BerriAI/<#readme-ov-file>:  Readme \\n⟨204⟩ https://github.com/BerriAI/<#License-1-ov-file>:  View license \\n⟨205⟩ https://github.com/BerriAI/<#security-ov-file>:  Security policy \\n⟨206⟩ https://github.com/BerriAI/</BerriAI/litellm/custom-properties>:  Custom properties\\n⟨207⟩ https://github.com/BerriAI/</BerriAI/litellm/watchers>:  **95** watching\\n⟨208⟩ https://github.com/BerriAI/</contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm&report=BerriAI+%28user%29>:  Report repository \\n⟨209⟩ https://github.com/BerriAI/</BerriAI/litellm/releases>: Releases 764\\n⟨210⟩ https://github.com/BerriAI/</BerriAI/litellm/releases/tag/v1.61.9-nightly>:  v1.61.9-nightly Latest  Feb 19, 2025 \\n⟨211⟩ https://github.com/BerriAI/</orgs/BerriAI/packages?repo_name=litellm>: Packages 0\\n⟨212⟩ https://github.com/BerriAI/</BerriAI/litellm/network/dependents>: Used by 7.1k\\n⟨213⟩ https://avatars.githubusercontent.com/u/51038655?s=64&v=4: \\n  * ![@magicKraken\\n⟨214⟩ https://avatars.githubusercontent.com/u/147166467?s=64&v=4: @jaimin-simform\\n⟨215⟩ https://avatars.githubusercontent.com/u/199928601?s=64&v=4: @Joel-moraes\\n⟨216⟩ https://avatars.githubusercontent.com/u/198551118?s=64&v=4: @JohannesBeckerr\\n⟨217⟩ https://avatars.githubusercontent.com/u/4981487?s=64&v=4: @zwy\\n⟨218⟩ https://avatars.githubusercontent.com/u/153007529?s=64&v=4: @krishshah9944\\n⟨219⟩ https://avatars.githubusercontent.com/u/181817643?s=64&v=4: @TylerDurden120\\n⟨220⟩ https://avatars.githubusercontent.com/u/164622679?s=64&v=4: @Wintoo12\\n⟨221⟩ https://github.com/BerriAI/</BerriAI/litellm/graphs/contributors>: Contributors 437\\n⟨222⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=python>:  Python 92.8% \\n⟨223⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=typescript>:  TypeScript 6.3% \\n⟨224⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=html>:  HTML 0.7% \\n⟨225⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=javascript>:  JavaScript 0.2% \\n⟨226⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=shell>:  Shell 0.0% \\n⟨227⟩ https://github.com/BerriAI/</BerriAI/litellm/search?l=dockerfile>:  Dockerfile 0.0% \\n⟨228⟩ https://github.com/BerriAI/<https:/github.com>: GitHub -  \\n⟨229⟩ https://github.com/BerriAI/<https:/docs.github.com/site-policy/github-terms/github-terms-of-service>: Terms\\n⟨230⟩ https://github.com/BerriAI/<https:/docs.github.com/site-policy/privacy-policies/github-privacy-statement>: Privacy\\n⟨231⟩ https://github.com/BerriAI/<https:/github.com/security>: Security\\n⟨232⟩ https://github.com/BerriAI/<https:/www.githubstatus.com/>: Status\\n⟨233⟩ https://github.com/BerriAI/<https:/docs.github.com/>: Docs\\n⟨234⟩ https://github.com/BerriAI/<https:/support.github.com?tags=dotcom-footer>: Contact\\n' fit_markdown='' fit_html=''",
  "generated_description": "BerriAI is a fledgling startup founded in 2023 and is a proud member of Y Combinator’s Winter 2023 batch. With a small but ambitious team of two, BerriAI aims to democratize access to large language models (LLMs) by providing a streamlined solution to call over 100 LLM APIs as if they're from OpenAI. Think of them as the universal translator for language models.\n\nTheir flagship product, **LiteLLM**, is an open-source library that simplifies the process of interacting with various LLM APIs, making it easier for developers to incorporate AI capabilities into their applications. By abstracting the complexities, LiteLLM allows users to manage API requests for providers like Azure, Google Cloud (VertexAI), Cohere, Hugging Face, and more—all under a unified interface.\n\nBerriAI is geared towards those looking to harness the power of multiple LLMs without getting bogged down by integration complications. They’ve designed LiteLLM to not only support traditional completion and embedding calls but also to handle features like rate limiting and cost tracking, which are crucial for enterprises managing multiple AI model integrations.\n\nThe founders have expressed a clear vision derived from a personal pain point—balancing the growing complexity and capabilities of various LLMs. They've recognized the market's need for a solution that keeps things simple while still offering extensive functionality.\n\nIf you’re intrigued, BerriAI’s code and documentation can be found on their GitHub repository: [BerriAI/litellm](https://github.com/BerriAI/litellm). \n\nIn the bustling world of AI startups, BerriAI stands out with a straightforward mission: make calling every LLM API feel as seamless as using OpenAI. For developers hovering at the intersection of innovation and practicality, this could be a game-changer. Keep an eye out for this company as they scale up and refine their offering in the growing AI landscape."
}