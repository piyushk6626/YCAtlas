{
  "links": "https://www.ycombinator.com/companies/downlink",
  "name": "Downlink",
  "headline": "Downlink makes LLMs 70%+ faster with a few lines of code",
  "batch": "W24",
  "description": "Downlink is the turbo button for AI. We make LLMs 70%+ faster with a few lines of code.\r\n\nDownlink learns which prompts are easy or hard. We train fast models to handle simple prompts, your existing LLM takes the complex ones.",
  "activity_status": "Active",
  "website": "https://downlink.dev/",
  "founded_date": 2024.0,
  "team_size": 1.0,
  "location": "San Francisco",
  "group_partner": "Nicolas Dessaigne",
  "group_partner_yc": "https://www.ycombinator.com/people/nicolas-dessaigne",
  "company_linkedin": null,
  "company_twitter": null,
  "tags": "industry:developer-tools; industry:saas; industry:api; industry:ai; location:san-francisco-bay-area",
  "founders": [
    {
      "name": "TJ Murphy, Founder",
      "description": "Downlink. The turbo button for AI.\n\nI build internet-scale data infrastructure.",
      "linkedin": "https://linkedin.com/in/tjmurphy"
    }
  ],
  "status": true,
  "markdown": "raw_markdown='[login](https://downlink.dev/</login>) >>>>\\ndownlink.\\n# The turbo button for AI\\nDownlink is a drop-in API to speed up your cloud LLMs. Use AI without tradeoffs, deploy to prod with a few lines of code.\\n[Book a demo](https://downlink.dev/<https:/calendly.com/_teej/downlink-demo>)\\n## Boost performance by 20% or more\\nExpand rate limits, lower response times, and reduce costs without sacrificing accuracy. \\n## Made for busy AI engineers\\nDownlink helps manage latency so you can spend more time shipping. \\n## Keep your architecture simple\\nBuild AI apps with less complexity. Reduce technical debt and maintenance burden with a simple API. \\n# Fine-tune withoutthe fuss\\nDownlink selects the best models and fine-tunes them to your use case. \\n# Get started in minutes\\nWorks with any language, use your existing client.\\nPython\\nTypeScript\\nGo\\nCurl\\n` import openai openai.base_url = \"https://app.downlink.dev/api/v1\" openai.api_key = \"__YOUR_DOWNLINK_API_KEY__\" response = openai.chat.completions.create(   model=\"gpt-4o\",   messages=[     {\"role\": \"user\", \"content\": \"Say \\'Hello World\\'\"}   ] ) print(response.choices[0].message.content) # => \"Hello World\" ` ` import OpenAI from \\'openai\\' const client = new OpenAI({  baseURL: \"https://app.downlink.dev/api/v1\",  apiKey: \"__YOUR_DOWNLINK_API_KEY__\" }) const response = await client.chat.completions.create({  model: \"gpt-4o\",  messages: [   {\"role\": \"user\", \"content\": \"Say \\'Hello World\\'\"}  ] }) console.log(response.choices[0].message.content) // => \"Hello World\" ` ` package main import (   \"context\" \"fmt\"   openai \"github.com/sashabaranov/go-openai\" ) func main() {   client := openai.NewClient(\"__YOUR_DOWNLINK_API_KEY__\")   client.BaseURL = \"https://app.downlink.dev/api/v1\"   resp, err := client.CreateChatCompletion(     context.Background(),     openai.ChatCompletionRequest{       Model: \"gpt-4o\",       Messages: []openai.ChatCompletionMessage{         {           Role: \"user\",           Content: \"Say \\'Hello World\\'\",         },       },     },   )   fmt.Println(resp.Choices[0].Message.Content) // => \"Hello World\" }     ` ` curl -n -X POST https://app.downlink.dev/api/v1/chat/completions \\\\  -H \"Authorization: Bearer __YOUR_DOWNLINK_API_KEY__\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{   \"model\": \"gpt-4o\",   \"messages\": [    {     \"role\": \"user\",     \"content\": \"Say \\'\\\\\\'\\'Hello World\\'\\\\\\'\\'\"    }   ]  }\\'     `\\n### downlink.\\n### /src/dev\\n[Playground](https://downlink.dev/<#>) [API Docs](https://downlink.dev/<#>)\\n### /var/info\\n[Team](https://downlink.dev/<#>) [Pricing](https://downlink.dev/<#>)\\n### /etc/connect\\n[LinkedIn](https://downlink.dev/<https:/www.linkedin.com/company/downlink/>) [Twitter](https://downlink.dev/<https:/x.com/_downlink>)\\n>>>> hello@downlink.dev deus ex machina >>>>\\n' markdown_with_citations='login⟨1⟩ >>>>\\ndownlink.\\n# The turbo button for AI\\nDownlink is a drop-in API to speed up your cloud LLMs. Use AI without tradeoffs, deploy to prod with a few lines of code.\\nBook a demo⟨2⟩\\n## Boost performance by 20% or more\\nExpand rate limits, lower response times, and reduce costs without sacrificing accuracy. \\n## Made for busy AI engineers\\nDownlink helps manage latency so you can spend more time shipping. \\n## Keep your architecture simple\\nBuild AI apps with less complexity. Reduce technical debt and maintenance burden with a simple API. \\n# Fine-tune withoutthe fuss\\nDownlink selects the best models and fine-tunes them to your use case. \\n# Get started in minutes\\nWorks with any language, use your existing client.\\nPython\\nTypeScript\\nGo\\nCurl\\n` import openai openai.base_url = \"https://app.downlink.dev/api/v1\" openai.api_key = \"__YOUR_DOWNLINK_API_KEY__\" response = openai.chat.completions.create(   model=\"gpt-4o\",   messages=[     {\"role\": \"user\", \"content\": \"Say \\'Hello World\\'\"}   ] ) print(response.choices[0].message.content) # => \"Hello World\" ` ` import OpenAI from \\'openai\\' const client = new OpenAI({  baseURL: \"https://app.downlink.dev/api/v1\",  apiKey: \"__YOUR_DOWNLINK_API_KEY__\" }) const response = await client.chat.completions.create({  model: \"gpt-4o\",  messages: [   {\"role\": \"user\", \"content\": \"Say \\'Hello World\\'\"}  ] }) console.log(response.choices[0].message.content) // => \"Hello World\" ` ` package main import (   \"context\" \"fmt\"   openai \"github.com/sashabaranov/go-openai\" ) func main() {   client := openai.NewClient(\"__YOUR_DOWNLINK_API_KEY__\")   client.BaseURL = \"https://app.downlink.dev/api/v1\"   resp, err := client.CreateChatCompletion(     context.Background(),     openai.ChatCompletionRequest{       Model: \"gpt-4o\",       Messages: []openai.ChatCompletionMessage{         {           Role: \"user\",           Content: \"Say \\'Hello World\\'\",         },       },     },   )   fmt.Println(resp.Choices[0].Message.Content) // => \"Hello World\" }     ` ` curl -n -X POST https://app.downlink.dev/api/v1/chat/completions \\\\  -H \"Authorization: Bearer __YOUR_DOWNLINK_API_KEY__\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{   \"model\": \"gpt-4o\",   \"messages\": [    {     \"role\": \"user\",     \"content\": \"Say \\'\\\\\\'\\'Hello World\\'\\\\\\'\\'\"    }   ]  }\\'     `\\n### downlink.\\n### /src/dev\\nPlayground⟨3⟩ API Docs⟨3⟩\\n### /var/info\\nTeam⟨3⟩ Pricing⟨3⟩\\n### /etc/connect\\nLinkedIn⟨4⟩ Twitter⟨5⟩\\n>>>> hello@downlink.dev deus ex machina >>>>\\n' references_markdown='\\n\\n## References\\n\\n⟨1⟩ https://downlink.dev/</login>: login\\n⟨2⟩ https://downlink.dev/<https:/calendly.com/_teej/downlink-demo>: Book a demo\\n⟨3⟩ https://downlink.dev/<#>: Playground\\n⟨4⟩ https://downlink.dev/<https:/www.linkedin.com/company/downlink/>: LinkedIn\\n⟨5⟩ https://downlink.dev/<https:/x.com/_downlink>: Twitter\\n' fit_markdown='' fit_html=''",
  "generated_description": "### Downlink: Turbocharging AI LLMs\n\nFounded in 2024 and part of Y Combinator Batch W24, Downlink is a young yet ambitious startup based in San Francisco. Imagine being able to make your Language Learning Models (LLMs) run 70% faster with just a few lines of code—that's exactly what Downlink promises. They call themselves the “turbo button for AI,” aiming to streamline AI application development without compromising performance.\n\n#### What's Their Mission?\n\nDownlink's mission is straightforward: to expedite the processing of LLMs significantly while keeping the integration process remarkably simple for developers. They achieve this by intelligently determining which prompts are straightforward and which ones require more computational power. Their solution intelligently routes less complex requests to faster models while your current LLM handles the more intricate queries.\n\n#### Solutions for Developers\n\nDownlink enhances your AI infrastructure by:\n- **Boosting Performance**: Experience a performance increase of 20% or more without sacrificing accuracy, lowering response times, and expanding rate limits.\n- **Simplifying Architecture**: Their drop-in API means less technical debt and complexity, allowing developers to focus on shipping rather than maintenance.\n- **Getting Started Quickly**: Their service works seamlessly with various languages like Python, TypeScript, and Go, meaning you can plug it into your existing stack with ease. \n\nFor example, a simple use case in Python looks like this:\n\n```python\nimport openai\n\nopenai.base_url = \"https://app.downlink.dev/api/v1\"\nopenai.api_key = \"__YOUR_DOWNLINK_API_KEY__\"\n\nresponse = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Say 'Hello World'\"}]\n)\n\nprint(response.choices[0].message.content) # Output: \"Hello World\"\n```\n\n#### A Lean Team, Big Ambitions\n\nThough currently there’s just one employee, Downlink is built on a solid vision and aspirations. Led by their Group Partner, Nicolas Dessaigne, they’re poised to redefine how companies leverage AI in their applications. \n\nIf you're an AI engineer tired of wrestling with latency and complex architectures, Downlink offers a much-needed lifeline. Check them out at their website [downlink.dev](https://downlink.dev/) to see how they can optimize your AI models.\n\nWith Downlink, you’re not just keeping pace; you’re setting the pace."
}